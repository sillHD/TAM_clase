{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":114239,"databundleVersionId":13825858,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:17:54.432537Z","iopub.execute_input":"2025-10-22T03:17:54.432888Z","iopub.status.idle":"2025-10-22T03:17:54.819487Z","shell.execute_reply.started":"2025-10-22T03:17:54.432863Z","shell.execute_reply":"2025-10-22T03:17:54.818698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport pickle\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nwarnings.filterwarnings('ignore')\n\n# ============================================================================\n# GPU CONFIGURATION & OPTIMIZATION\n# ============================================================================\n\ndef setup_gpu():\n    \"\"\"Configure GPU for optimal performance\"\"\"\n    print(\"=\"*80)\n    print(\"GPU CONFIGURATION\")\n    print(\"=\"*80)\n    \n    # Check available GPUs\n    gpus = tf.config.list_physical_devices('GPU')\n    print(f\"\\nüñ•Ô∏è  Available GPUs: {len(gpus)}\")\n    \n    if gpus:\n        try:\n            # Enable memory growth (don't allocate all GPU memory at once)\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n                print(f\"   ‚úì GPU: {gpu.name} - Memory growth enabled\")\n            \n            # Set GPU memory limit (optional - useful if sharing GPU)\n            # tf.config.set_logical_device_configuration(\n            #     gpus[0],\n            #     [tf.config.LogicalDeviceConfiguration(memory_limit=4096)]  # 4GB\n            # )\n            \n            # Use mixed precision for faster training\n            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n            tf.keras.mixed_precision.set_global_policy(policy)\n            print(f\"   ‚úì Mixed precision enabled: {policy.name}\")\n            \n            logical_gpus = tf.config.list_logical_devices('GPU')\n            print(f\"   ‚úì Logical GPUs: {len(logical_gpus)}\")\n            \n        except RuntimeError as e:\n            print(f\"   ‚ö†Ô∏è  GPU configuration error: {e}\")\n    else:\n        print(\"   ‚ö†Ô∏è  No GPU found - using CPU (training will be slower)\")\n    \n    # Set TensorFlow options for better performance\n    tf.config.optimizer.set_jit(True)  # XLA compilation\n    print(\"   ‚úì XLA (Accelerated Linear Algebra) enabled\")\n    \n    print(f\"\\nüìä TensorFlow version: {tf.__version__}\")\n    print(f\"üìä Keras version: {keras.__version__}\")\n    \n    return len(gpus) > 0\n\n# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\nCONFIG = {\n    'sequence_length': 10,\n    'max_frames_to_predict': 15,\n    'batch_size': 256,  # Larger batch for GPU\n    'epochs': 100,\n    'learning_rate': 0.001,\n    'validation_split': 0.15,\n    'use_gpu': True,\n}\n\n# ============================================================================\n# EVALUATION METRICS\n# ============================================================================\n\ndef calculate_rmse(y_true, y_pred):\n    \"\"\"Calculate Root Mean Squared Error\"\"\"\n    mse = np.mean((y_true - y_pred) ** 2)\n    rmse = np.sqrt(mse)\n    return rmse\n\ndef calculate_mae(y_true, y_pred):\n    \"\"\"Calculate Mean Absolute Error\"\"\"\n    mae = np.mean(np.abs(y_true - y_pred))\n    return mae\n\ndef calculate_euclidean_distance(y_true, y_pred):\n    \"\"\"Calculate Euclidean distance between predicted and actual positions\"\"\"\n    distances = np.sqrt((y_true[:, 0] - y_pred[:, 0])**2 + \n                       (y_true[:, 1] - y_pred[:, 1])**2)\n    return distances\n\ndef evaluate_predictions(y_true, y_pred, split_name=\"Validation\"):\n    \"\"\"Comprehensive evaluation of predictions\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(f\"üìä {split_name.upper()} SET EVALUATION\")\n    print(\"=\"*80)\n    \n    # Overall metrics\n    x_rmse = calculate_rmse(y_true[:, 0], y_pred[:, 0])\n    y_rmse = calculate_rmse(y_true[:, 1], y_pred[:, 1])\n    \n    x_mae = calculate_mae(y_true[:, 0], y_pred[:, 0])\n    y_mae = calculate_mae(y_true[:, 1], y_pred[:, 1])\n    \n    # Euclidean distance\n    distances = calculate_euclidean_distance(y_true, y_pred)\n    mean_distance = np.mean(distances)\n    median_distance = np.median(distances)\n    \n    print(f\"\\nüéØ POSITION ACCURACY:\")\n    print(f\"   X-coordinate:\")\n    print(f\"      RMSE: {x_rmse:.3f} yards\")\n    print(f\"      MAE:  {x_mae:.3f} yards\")\n    \n    print(f\"\\n   Y-coordinate:\")\n    print(f\"      RMSE: {y_rmse:.3f} yards\")\n    print(f\"      MAE:  {y_mae:.3f} yards\")\n    \n    print(f\"\\nüìè EUCLIDEAN DISTANCE:\")\n    print(f\"   Mean:   {mean_distance:.3f} yards\")\n    print(f\"   Median: {median_distance:.3f} yards\")\n    print(f\"   Std:    {np.std(distances):.3f} yards\")\n    print(f\"   Min:    {np.min(distances):.3f} yards\")\n    print(f\"   Max:    {np.max(distances):.3f} yards\")\n    \n    # Percentiles\n    print(f\"\\nüìä DISTANCE PERCENTILES:\")\n    for p in [25, 50, 75, 90, 95, 99]:\n        print(f\"   {p}th percentile: {np.percentile(distances, p):.3f} yards\")\n    \n    # Accuracy buckets\n    print(f\"\\nüéØ ACCURACY BUCKETS:\")\n    for threshold in [1, 2, 5, 10, 15, 20]:\n        within = (distances <= threshold).sum()\n        pct = 100 * within / len(distances)\n        print(f\"   Within {threshold:2d} yards: {within:6,} ({pct:5.2f}%)\")\n    \n    metrics = {\n        'x_rmse': x_rmse,\n        'y_rmse': y_rmse,\n        'x_mae': x_mae,\n        'y_mae': y_mae,\n        'mean_distance': mean_distance,\n        'median_distance': median_distance,\n        'distances': distances\n    }\n    \n    return metrics\n\ndef plot_predictions(y_true, y_pred, split_name=\"Validation\", save_path=\"predictions_plot.png\"):\n    \"\"\"Visualize predictions vs actual\"\"\"\n    \n    fig = plt.figure(figsize=(20, 12))\n    \n    # 1. X predictions scatter\n    ax1 = plt.subplot(2, 3, 1)\n    ax1.scatter(y_true[:, 0], y_pred[:, 0], alpha=0.3, s=1)\n    ax1.plot([0, 120], [0, 120], 'r--', linewidth=2)\n    ax1.set_xlabel('Actual X (yards)', fontsize=12)\n    ax1.set_ylabel('Predicted X (yards)', fontsize=12)\n    ax1.set_title(f'{split_name} - X Coordinate', fontsize=14, fontweight='bold')\n    ax1.grid(alpha=0.3)\n    \n    # 2. Y predictions scatter\n    ax2 = plt.subplot(2, 3, 2)\n    ax2.scatter(y_true[:, 1], y_pred[:, 1], alpha=0.3, s=1)\n    ax2.plot([0, 53.3], [0, 53.3], 'r--', linewidth=2)\n    ax2.set_xlabel('Actual Y (yards)', fontsize=12)\n    ax2.set_ylabel('Predicted Y (yards)', fontsize=12)\n    ax2.set_title(f'{split_name} - Y Coordinate', fontsize=14, fontweight='bold')\n    ax2.grid(alpha=0.3)\n    \n    # 3. Error distribution\n    ax3 = plt.subplot(2, 3, 3)\n    distances = calculate_euclidean_distance(y_true, y_pred)\n    ax3.hist(distances, bins=50, alpha=0.7, edgecolor='black')\n    ax3.axvline(np.mean(distances), color='red', linestyle='--', \n                linewidth=2, label=f'Mean: {np.mean(distances):.2f}')\n    ax3.set_xlabel('Euclidean Distance Error (yards)', fontsize=12)\n    ax3.set_ylabel('Frequency', fontsize=12)\n    ax3.set_title('Prediction Error Distribution', fontsize=14, fontweight='bold')\n    ax3.legend()\n    ax3.grid(alpha=0.3)\n    \n    # 4. X error distribution\n    ax4 = plt.subplot(2, 3, 4)\n    x_errors = y_true[:, 0] - y_pred[:, 0]\n    ax4.hist(x_errors, bins=50, alpha=0.7, edgecolor='black', color='green')\n    ax4.axvline(0, color='red', linestyle='--', linewidth=2)\n    ax4.set_xlabel('X Error (yards)', fontsize=12)\n    ax4.set_ylabel('Frequency', fontsize=12)\n    ax4.set_title(f'X Error - Mean: {np.mean(x_errors):.3f}', fontsize=14, fontweight='bold')\n    ax4.grid(alpha=0.3)\n    \n    # 5. Y error distribution\n    ax5 = plt.subplot(2, 3, 5)\n    y_errors = y_true[:, 1] - y_pred[:, 1]\n    ax5.hist(y_errors, bins=50, alpha=0.7, edgecolor='black', color='orange')\n    ax5.axvline(0, color='red', linestyle='--', linewidth=2)\n    ax5.set_xlabel('Y Error (yards)', fontsize=12)\n    ax5.set_ylabel('Frequency', fontsize=12)\n    ax5.set_title(f'Y Error - Mean: {np.mean(y_errors):.3f}', fontsize=14, fontweight='bold')\n    ax5.grid(alpha=0.3)\n    \n    # 6. Cumulative accuracy\n    ax6 = plt.subplot(2, 3, 6)\n    sorted_distances = np.sort(distances)\n    cumulative = np.arange(1, len(sorted_distances) + 1) / len(sorted_distances) * 100\n    ax6.plot(sorted_distances, cumulative, linewidth=2)\n    ax6.set_xlabel('Distance Threshold (yards)', fontsize=12)\n    ax6.set_ylabel('Cumulative % of Predictions', fontsize=12)\n    ax6.set_title('Cumulative Accuracy Curve', fontsize=14, fontweight='bold')\n    ax6.grid(alpha=0.3)\n    \n    # Add benchmarks\n    for threshold in [5, 10, 15]:\n        pct = (distances <= threshold).sum() / len(distances) * 100\n        ax6.axvline(threshold, linestyle='--', alpha=0.5)\n        ax6.text(threshold, pct, f'{pct:.1f}%', fontsize=10)\n    \n    plt.suptitle(f'{split_name} Set - Prediction Analysis', \n                 fontsize=16, fontweight='bold', y=0.995)\n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    print(f\"\\n‚úì Saved plot: {save_path}\")\n    \n    return fig\n\ndef plot_training_history(history, save_path=\"training_history.png\"):\n    \"\"\"Plot training history\"\"\"\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Loss plot\n    axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n    axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n    axes[0].set_xlabel('Epoch', fontsize=12)\n    axes[0].set_ylabel('Loss (MSE)', fontsize=12)\n    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n    axes[0].legend()\n    axes[0].grid(alpha=0.3)\n    \n    # MAE plot\n    axes[1].plot(history.history['mae'], label='Training MAE', linewidth=2)\n    axes[1].plot(history.history['val_mae'], label='Validation MAE', linewidth=2)\n    axes[1].set_xlabel('Epoch', fontsize=12)\n    axes[1].set_ylabel('MAE (yards)', fontsize=12)\n    axes[1].set_title('Training and Validation MAE', fontsize=14, fontweight='bold')\n    axes[1].legend()\n    axes[1].grid(alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    print(f\"‚úì Saved plot: {save_path}\")\n    \n    return fig\n\n# ============================================================================\n# IMPORT FUNCTIONS FROM ORIGINAL CODE\n# ============================================================================\n\ndef parse_height(height_str):\n    if pd.isna(height_str):\n        return np.nan\n    try:\n        feet, inches = map(int, str(height_str).split('-'))\n        return feet * 12 + inches\n    except:\n        return np.nan\n\ndef calculate_age(birth_date, reference_date='2023-09-01'):\n    try:\n        birth = pd.to_datetime(birth_date)\n        ref = pd.to_datetime(reference_date)\n        return (ref - birth).days / 365.25\n    except:\n        return np.nan\n\ndef load_training_data(data_path='/kaggle/input/nfl-big-data-bowl-2026-prediction/train'):\n    print(\"\\n\" + \"=\"*80)\n    print(\"LOADING TRAINING DATA\")\n    print(\"=\"*80)\n    \n    all_data = []\n    for week in range(1, 19):\n        file_path = f'{data_path}/input_2023_w{week:02d}.csv'\n        try:\n            df = pd.read_csv(file_path)\n            all_data.append(df)\n            print(f\"‚úì Week {week:02d}: {len(df):,} rows | {df['play_id'].nunique():,} plays\")\n        except FileNotFoundError:\n            print(f\"‚úó Week {week:02d}: File not found\")\n    \n    train_df = pd.concat(all_data, ignore_index=True)\n    print(f\"\\nTotal training data: {len(train_df):,} rows\")\n    print(f\" Unique plays: {(train_df['game_id'].astype(str) + '_' + train_df['play_id'].astype(str)).nunique():,}\")\n    print(f\"Players to predict: {train_df['player_to_predict'].sum():,}\")\n    \n    return train_df\n\ndef load_test_data():\n    print(\"\\n\" + \"=\"*80)\n    print(\"LOADING TEST DATA\")\n    print(\"=\"*80)\n    \n    test_input = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2026-prediction/test_input.csv')\n    test_targets = pd.read_csv('/kaggle/input/nfl-big-data-bowl-2026-prediction/test.csv')\n    \n    print(f\"‚úì Test input: {len(test_input):,} rows\")\n    print(f\"‚úì Test targets: {len(test_targets):,} predictions needed\")\n    \n    return test_input, test_targets\n\ndef normalize_play_direction(df):\n    df = df.copy()\n    left_mask = df['play_direction'] == 'left'\n    num_flipped = left_mask.sum()\n    \n    df.loc[left_mask, 'x'] = 120 - df.loc[left_mask, 'x']\n    df.loc[left_mask, 'y'] = 53.3 - df.loc[left_mask, 'y']\n    df.loc[left_mask, 'dir'] = (df.loc[left_mask, 'dir'] + 180) % 360\n    df.loc[left_mask, 'o'] = (df.loc[left_mask, 'o'] + 180) % 360\n    \n    if 'ball_land_x' in df.columns:\n        df.loc[left_mask, 'ball_land_x'] = 120 - df.loc[left_mask, 'ball_land_x']\n        df.loc[left_mask, 'ball_land_y'] = 53.3 - df.loc[left_mask, 'ball_land_y']\n    \n    print(f\"   Normalized {num_flipped:,} plays moving left ‚Üí right\")\n    return df\n\ndef engineer_features(df):\n    print(\"\\n\" + \"=\"*80)\n    print(\"FEATURE ENGINEERING\")\n    print(\"=\"*80)\n    \n    df = df.copy()\n    \n    print(\"‚úì Computing velocity components (vx, vy)\")\n    df['vx'] = df['s'] * np.cos(np.radians(df['dir']))\n    df['vy'] = df['s'] * np.sin(np.radians(df['dir']))\n    \n    print(\"‚úì Computing orientation components (ox, oy)\")\n    df['ox'] = np.cos(np.radians(df['o']))\n    df['oy'] = np.sin(np.radians(df['o']))\n    \n    if 'ball_land_x' in df.columns:\n        print(\"‚úì Computing ball landing features\")\n        df['dist_to_ball'] = np.sqrt(\n            (df['x'] - df['ball_land_x'])**2 + \n            (df['y'] - df['ball_land_y'])**2\n        )\n        df['angle_to_ball'] = np.arctan2(\n            df['ball_land_y'] - df['y'],\n            df['ball_land_x'] - df['x']\n        )\n        df['vel_toward_ball'] = df['s'] * np.cos(np.radians(df['dir']) - df['angle_to_ball'])\n    else:\n        df['dist_to_ball'] = 0\n        df['angle_to_ball'] = 0\n        df['vel_toward_ball'] = 0\n    \n    print(\"‚úì Computing field position features\")\n    df['dist_to_left_sideline'] = df['y']\n    df['dist_to_right_sideline'] = 53.3 - df['y']\n    df['dist_to_nearest_sideline'] = np.minimum(df['y'], 53.3 - df['y'])\n    df['dist_to_endzone'] = 120 - df['x']\n    \n    print(\"‚úì Processing player attributes\")\n    df['height_inches'] = df['player_height'].apply(parse_height)\n    df['height_inches'] = df['height_inches'].fillna(df['height_inches'].median())\n    \n    df['player_age'] = df['player_birth_date'].apply(calculate_age)\n    df['player_age'] = df['player_age'].fillna(df['player_age'].median())\n    \n    df['bmi'] = (df['player_weight'] * 703) / (df['height_inches'] ** 2)\n    df['bmi'] = df['bmi'].fillna(df['bmi'].median())\n    \n    print(\"‚úì Creating temporal features (lags, differences)\")\n    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n    \n    group_cols = ['game_id', 'play_id', 'nfl_id']\n    for lag in [1, 2, 3]:\n        for col in ['x', 'y', 's', 'a', 'vx', 'vy']:\n            df[f'{col}_lag{lag}'] = df.groupby(group_cols)[col].shift(lag)\n    \n    df['speed_change'] = df.groupby(group_cols)['s'].diff()\n    df['accel_change'] = df.groupby(group_cols)['a'].diff()\n    df['dir_change'] = df.groupby(group_cols)['dir'].diff()\n    \n    df.loc[df['dir_change'] > 180, 'dir_change'] -= 360\n    df.loc[df['dir_change'] < -180, 'dir_change'] += 360\n    \n    print(\"‚úì Computing rolling statistics\")\n    for col in ['s', 'a']:\n        df[f'{col}_roll_mean'] = df.groupby(group_cols)[col].transform(\n            lambda x: x.rolling(window=3, min_periods=1).mean()\n        )\n        df[f'{col}_roll_std'] = df.groupby(group_cols)[col].transform(\n            lambda x: x.rolling(window=3, min_periods=1).std()\n        )\n    \n    df = df.fillna(method='bfill').fillna(method='ffill').fillna(0)\n    \n    print(f\"\\nüìä Features created: {len(df.columns)} total columns\")\n    \n    return df\n\ndef encode_categorical(df, encoders=None):\n    df = df.copy()\n    categorical_cols = ['player_position', 'player_side', 'player_role']\n    \n    if encoders is None:\n        encoders = {}\n        for col in categorical_cols:\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col].astype(str))\n            encoders[col] = le\n        return df, encoders\n    else:\n        for col in categorical_cols:\n            if col in encoders:\n                df[col] = df[col].astype(str).map(\n                    lambda x: x if x in encoders[col].classes_ else encoders[col].classes_[0]\n                )\n                df[col] = encoders[col].transform(df[col])\n        return df\n\ndef create_sequences(df, sequence_length=10, for_training=True):\n    print(\"\\n\" + \"=\"*80)\n    print(\"CREATING SEQUENCES\")\n    print(\"=\"*80)\n    \n    sequence_features = [\n        'x', 'y', 's', 'a', 'vx', 'vy', 'ox', 'oy', 'dir', 'o',\n        'x_lag1', 'y_lag1', 's_lag1', 'a_lag1',\n        'x_lag2', 'y_lag2', 's_lag2', 'a_lag2',\n        'x_lag3', 'y_lag3', 's_lag3', 'a_lag3',\n        'speed_change', 'accel_change', 'dir_change',\n        's_roll_mean', 'a_roll_mean',\n        'dist_to_left_sideline', 'dist_to_right_sideline', 'dist_to_nearest_sideline'\n    ]\n    \n    static_features = [\n        'player_position', 'player_side', 'player_role',\n        'height_inches', 'player_weight', 'player_age', 'bmi',\n        'absolute_yardline_number', 'dist_to_ball', 'angle_to_ball'\n    ]\n    \n    sequences = []\n    static_data = []\n    targets = []\n    metadata = []\n    \n    grouped = df.groupby(['game_id', 'play_id', 'nfl_id'])\n    \n    for (game_id, play_id, nfl_id), group in grouped:\n        if for_training and not group['player_to_predict'].any():\n            continue\n        \n        group = group.sort_values('frame_id')\n        \n        if len(group) < sequence_length:\n            continue\n        \n        seq_data = group[sequence_features].iloc[-sequence_length:].values\n        static = group[static_features].iloc[-1].values\n        \n        sequences.append(seq_data)\n        static_data.append(static)\n        \n        if for_training and 'ball_land_x' in group.columns:\n            target_x = group['ball_land_x'].iloc[-1]\n            target_y = group['ball_land_y'].iloc[-1]\n            targets.append([target_x, target_y])\n        \n        metadata.append({\n            'game_id': game_id,\n            'play_id': play_id,\n            'nfl_id': nfl_id,\n            'num_frames_output': group['num_frames_output'].iloc[-1] if 'num_frames_output' in group.columns else 0,\n            'last_x': group['x'].iloc[-1],\n            'last_y': group['y'].iloc[-1],\n        })\n    \n    sequences = np.array(sequences, dtype=np.float32)\n    static_data = np.array(static_data, dtype=np.float32)\n    \n    if for_training and len(targets) > 0:\n        targets = np.array(targets, dtype=np.float32)\n    else:\n        targets = None\n    \n    print(f\"‚úì Created {len(sequences):,} sequences\")\n    print(f\"‚úì Sequence shape: {sequences.shape}\")\n    print(f\"‚úì Static shape: {static_data.shape}\")\n    if targets is not None:\n        print(f\"‚úì Target shape: {targets.shape}\")\n    \n    return sequences, static_data, targets, metadata\n\ndef build_model(sequence_shape, static_shape):\n    print(\"\\n\" + \"=\"*80)\n    print(\"BUILDING MODEL\")\n    print(\"=\"*80)\n    \n    sequence_input = layers.Input(shape=sequence_shape, name='sequence_input')\n    \n    x = layers.LSTM(128, return_sequences=True)(sequence_input)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    x = layers.LSTM(64, return_sequences=False)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.3)(x)\n    \n    static_input = layers.Input(shape=(static_shape,), name='static_input')\n    s = layers.Dense(64, activation='relu')(static_input)\n    s = layers.BatchNormalization()(s)\n    s = layers.Dropout(0.2)(s)\n    s = layers.Dense(32, activation='relu')(s)\n    \n    combined = layers.concatenate([x, s])\n    \n    z = layers.Dense(128, activation='relu')(combined)\n    z = layers.BatchNormalization()(z)\n    z = layers.Dropout(0.3)(z)\n    \n    z = layers.Dense(64, activation='relu')(z)\n    z = layers.Dropout(0.2)(z)\n    \n    # For mixed precision, use float32 output\n    output = layers.Dense(2, dtype='float32', name='position_output')(z)\n    \n    model = keras.Model(\n        inputs=[sequence_input, static_input],\n        outputs=output\n    )\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=CONFIG['learning_rate']),\n        loss='mse',\n        metrics=['mae', 'mse']\n    )\n    \n    model.summary()\n    \n    return model\n\ndef train_model(model, X_seq, X_static, y, validation_split=0.15):\n    print(\"\\n\" + \"=\"*80)\n    print(\"TRAINING MODEL\")\n    print(\"=\"*80)\n    \n    callbacks = [\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=15,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=7,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        keras.callbacks.ModelCheckpoint(\n            'best_model.keras',\n            monitor='val_loss',\n            save_best_only=True,\n            verbose=1\n        )\n    ]\n    \n    history = model.fit(\n        [X_seq, X_static], y,\n        batch_size=CONFIG['batch_size'],\n        epochs=CONFIG['epochs'],\n        validation_split=validation_split,\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    return model, history\n\ndef create_submission(model, test_input, test_targets, metadata_lookup, scalers):\n    print(\"\\n\" + \"=\"*80)\n    print(\"GENERATING PREDICTIONS\")\n    print(\"=\"*80)\n    \n    pred_dict = {}\n    for meta, pred in zip(metadata_lookup, model.predict([test_input[0], test_input[1]], verbose=1)):\n        key = (meta['game_id'], meta['play_id'], meta['nfl_id'])\n        pred_dict[key] = {\n            'x': pred[0],\n            'y': pred[1],\n            'last_x': meta['last_x'],\n            'last_y': meta['last_y']\n        }\n    \n    submissions = []\n    for _, row in test_targets.iterrows():\n        key = (row['game_id'], row['play_id'], row['nfl_id'])\n        \n        if key in pred_dict:\n            x_pred = pred_dict[key]['x']\n            y_pred = pred_dict[key]['y']\n        else:\n            x_pred = 60.0\n            y_pred = 26.65\n        \n        submissions.append({\n            'id': f\"{row['game_id']}_{row['play_id']}_{row['nfl_id']}_{row['frame_id']}\",\n            'x': x_pred,\n            'y': y_pred\n        })\n    \n    submission_df = pd.DataFrame(submissions)\n    submission_df.to_csv('submission.csv', index=False)\n    \n    print(f\"‚úì Submission created: {len(submission_df):,} predictions\")\n    print(f\"‚úì Saved to: submission.csv\")\n    \n    return submission_df\n\n# ============================================================================\n# MAIN PIPELINE WITH EVALUATION\n# ============================================================================\n'''\ndef main():\n    start_time = datetime.now()\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\" NFL BIG DATA BOWL 2026 - ENHANCED PIPELINE WITH EVALUATION\")\n    print(\"=\"*80)\n    \n    # Setup GPU\n    has_gpu = setup_gpu()\n    \n    # Load data\n    train_df = load_training_data()\n    test_input_df, test_targets_df = load_test_data()\n    \n    # Preprocess\n    print(\"\\nüìç Step 1: Normalizing play direction...\")\n    train_df = normalize_play_direction(train_df)\n    test_input_df = normalize_play_direction(test_input_df)\n    \n    # Feature engineering\n    print(\"\\nüìç Step 2: Feature engineering...\")\n    train_df = engineer_features(train_df)\n    test_input_df = engineer_features(test_input_df)\n    \n    # Encode categorical\n    print(\"\\nüìç Step 3: Encoding categorical variables...\")\n    train_df, encoders = encode_categorical(train_df)\n    test_input_df = encode_categorical(test_input_df, encoders)\n    \n    # Create sequences\n    print(\"\\nüìç Step 4: Creating sequences...\")\n    X_seq_all, X_static_all, y_all, metadata_all = create_sequences(\n        train_df, CONFIG['sequence_length'], for_training=True\n    )\n    \n    X_seq_test, X_static_test, _, metadata_test = create_sequences(\n        test_input_df, CONFIG['sequence_length'], for_training=False\n    )\n    \n    # Split train/validation\n    print(\"\\nüìç Step 5: Splitting train/validation...\")\n    n_samples = len(X_seq_all)\n    n_val = int(n_samples * CONFIG['validation_split'])\n    \n    # Random shuffle\n    indices = np.random.permutation(n_samples)\n    train_idx = indices[n_val:]\n    val_idx = indices[:n_val]\n    \n    X_seq_train = X_seq_all[train_idx]\n    X_static_train = X_static_all[train_idx]\n    y_train = y_all[train_idx]\n    \n    X_seq_val = X_seq_all[val_idx]\n    X_static_val = X_static_all[val_idx]\n    y_val = y_all[val_idx]\n    \n    print(f\"   Training samples: {len(X_seq_train):,}\")\n    print(f\"   Validation samples: {len(X_seq_val):,}\")\n    \n    # Scale features\n    print(\"\\nüìç Step 6: Scaling features...\")\n    scaler_seq = StandardScaler()\n    scaler_static = StandardScaler()\n    \n    # Scale sequence features\n    X_seq_train_flat = X_seq_train.reshape(-1, X_seq_train.shape[-1])\n    X_seq_train_scaled = scaler_seq.fit_transform(X_seq_train_flat).reshape(X_seq_train.shape)\n    \n    X_seq_val_flat = X_seq_val.reshape(-1, X_seq_val.shape[-1])\n    X_seq_val_scaled = scaler_seq.transform(X_seq_val_flat).reshape(X_seq_val.shape)\n    \n    # Scale static features\n    X_static_train_scaled = scaler_static.fit_transform(X_static_train)\n    X_static_val_scaled = scaler_static.transform(X_static_val)\n    \n    # Scale test features\n    X_seq_test_flat = X_seq_test.reshape(-1, X_seq_test.shape[-1])\n    X_seq_test_scaled = scaler_seq.transform(X_seq_test_flat).reshape(X_seq_test.shape)\n    X_static_test_scaled = scaler_static.transform(X_static_test)\n    \n    # Build model\n    print(\"\\nüìç Step 7: Building model...\")\n    model = build_model(\n        sequence_shape=(X_seq_train.shape[1], X_seq_train.shape[2]),\n        static_shape=X_static_train.shape[1]\n    )\n    \n    # Train model WITHOUT validation_split (we already split)\n    print(\"\\nüìç Step 8: Training model...\")\n    \n    callbacks = [\n        keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=15,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=7,\n            min_lr=1e-6,\n            verbose=1\n        ),\n        keras.callbacks.ModelCheckpoint(\n            'best_model.keras',\n            monitor='val_loss',\n            save_best_only=True,\n            verbose=1\n        )\n    ]\n    \n    history = model.fit(\n        [X_seq_train_scaled, X_static_train_scaled], y_train,\n        validation_data=([X_seq_val_scaled, X_static_val_scaled], y_val),\n        batch_size=CONFIG['batch_size'],\n        epochs=CONFIG['epochs'],\n        callbacks=callbacks,\n        verbose=1\n    )\n    \n    # Plot training history\n    print(\"\\nüìç Step 9: Plotting training history...\")\n    plot_training_history(history, \"training_history.png\")\n    \n    # Evaluate on training set\n    print(\"\\nüìç Step 10: Evaluating on training set...\")\n    y_train_pred = model.predict([X_seq_train_scaled, X_static_train_scaled], verbose=0)\n    train_metrics = evaluate_predictions(y_train, y_train_pred, \"Training\")\n    plot_predictions(y_train, y_train_pred, \"Training\", \"predictions_train.png\")\n    \n    # Evaluate on validation set\n    print(\"\\nüìç Step 11: Evaluating on validation set...\")\n    y_val_pred = model.predict([X_seq_val_scaled, X_static_val_scaled], verbose=0)\n    val_metrics = evaluate_predictions(y_val, y_val_pred, \"Validation\")\n    plot_predictions(y_val, y_val_pred, \"Validation\", \"predictions_val.png\")\n    \n    # Save model and artifacts\n    print(\"\\nüìç Step 12: Saving model and artifacts...\")\n    model.save('nfl_model_final.keras')\n    with open('scalers.pkl', 'wb') as f:\n        pickle.dump({'seq': scaler_seq, 'static': scaler_static, 'encoders': encoders}, f)\n    \n    # Save metrics to file\n    metrics_summary = {\n        'training': {\n            'x_rmse': float(train_metrics['x_rmse']),\n            'y_rmse': float(train_metrics['y_rmse']),\n            'x_mae': float(train_metrics['x_mae']),\n            'y_mae': float(train_metrics['y_mae']),\n            'mean_distance': float(train_metrics['mean_distance']),\n            'median_distance': float(train_metrics['median_distance'])\n        },\n        'validation': {\n            'x_rmse': float(val_metrics['x_rmse']),\n            'y_rmse': float(val_metrics['y_rmse']),\n            'x_mae': float(val_metrics['x_mae']),\n            'y_mae': float(val_metrics['y_mae']),\n            'mean_distance': float(val_metrics['mean_distance']),\n            'median_distance': float(val_metrics['median_distance'])\n        }\n    }\n    \n    with open('metrics.pkl', 'wb') as f:\n        pickle.dump(metrics_summary, f)\n    \n    # Create submission\n    print(\"\\nüìç Step 13: Creating submission...\")\n    submission = create_submission(\n        model, \n        (X_seq_test_scaled, X_static_test_scaled),\n        test_targets_df,\n        metadata_test,\n        {'seq': scaler_seq, 'static': scaler_static}\n    )\n    \n    # Final summary\n    end_time = datetime.now()\n    duration = end_time - start_time\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"‚úÖ PIPELINE COMPLETE!\")\n    print(\"=\"*80)\n    \n    print(f\"\\n‚è±Ô∏è  Total Time: {duration}\")\n    \n    print(f\"\\nüìÅ Files created:\")\n    print(f\"   ‚Ä¢ nfl_model_final.keras - Trained model\")\n    print(f\"   ‚Ä¢ best_model.keras - Best model checkpoint\")\n    print(f\"   ‚Ä¢ scalers.pkl - Feature scalers and encoders\")\n    print(f\"   ‚Ä¢ metrics.pkl - Evaluation metrics\")\n    print(f\"   ‚Ä¢ submission.csv - Final predictions ({len(submission):,} rows)\")\n    print(f\"   ‚Ä¢ training_history.png - Training curves\")\n    print(f\"   ‚Ä¢ predictions_train.png - Training set predictions\")\n    print(f\"   ‚Ä¢ predictions_val.png - Validation set predictions\")\n    \n    print(f\"\\nüìä FINAL RESULTS:\")\n    print(f\"\\n   Training Set:\")\n    print(f\"      RMSE (X): {train_metrics['x_rmse']:.3f} yards\")\n    print(f\"      RMSE (Y): {train_metrics['y_rmse']:.3f} yards\")\n    print(f\"      Mean Distance Error: {train_metrics['mean_distance']:.3f} yards\")\n    \n    print(f\"\\n   Validation Set:\")\n    print(f\"      RMSE (X): {val_metrics['x_rmse']:.3f} yards\")\n    print(f\"      RMSE (Y): {val_metrics['y_rmse']:.3f} yards\")\n    print(f\"      Mean Distance Error: {val_metrics['mean_distance']:.3f} yards\")\n    \n    print(f\"\\nüéØ Model Performance Summary:\")\n    within_5_val = (val_metrics['distances'] <= 5).sum() / len(val_metrics['distances']) * 100\n    within_10_val = (val_metrics['distances'] <= 10).sum() / len(val_metrics['distances']) * 100\n    print(f\"   Predictions within 5 yards: {within_5_val:.1f}%\")\n    print(f\"   Predictions within 10 yards: {within_10_val:.1f}%\")\n    \n    print(\"\\n\" + \"=\"*80)\n    \n    return model, history, submission, train_metrics, val_metrics\n\n# ============================================================================\n# RUN\n# ============================================================================\n\nif __name__ == \"__main__\":\n    model, history, submission, train_metrics, val_metrics = main()\n'''\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:17:54.821069Z","iopub.execute_input":"2025-10-22T03:17:54.821402Z","iopub.status.idle":"2025-10-22T03:18:04.182511Z","shell.execute_reply.started":"2025-10-22T03:17:54.821381Z","shell.execute_reply":"2025-10-22T03:18:04.181730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = load_training_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:18:04.183392Z","iopub.execute_input":"2025-10-22T03:18:04.183794Z","iopub.status.idle":"2025-10-22T03:18:25.791334Z","shell.execute_reply.started":"2025-10-22T03:18:04.183773Z","shell.execute_reply":"2025-10-22T03:18:25.790476Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# An√°lisis del conjunto de datos\n---\n## 1. Descripci√≥n general del dataset y del objetivo del concurso\nLa competencia **[NFL Big Data Bowl 2026](https://www.kaggle.com/competitions/nfl-big-data-bowl-2026-prediction)** es un reto anual organizado en colaboraci√≥n con la **National Football League (NFL)** y tiene como prop√≥sito impulsar la innovaci√≥n en el an√°lisis de datos deportivos, especialmente en **jugadas de f√∫tbol americano profesional**.\n\nEl objetivo es utilizar datos de **seguimiento de jugadores en el campo (player tracking)** junto con informaci√≥n contextual de jugadas, equipos y jugadores, para **predecir eventos o m√©tricas espec√≠ficas de rendimiento en tiempo real**. Este tipo de an√°lisis permite desarrollar modelos predictivos avanzados aplicables a estrategias de juego, scouting y an√°lisis estad√≠stico de alto nivel.\n\n---\n\n### Objetivo principal del concurso\n\nEl **objetivo central** es desarrollar modelos predictivos que **estimen resultados de jugadas ofensivas a partir de datos de posici√≥n y contexto**.  \nEn esta edici√≥n, los participantes deben:\n\n- Utilizar informaci√≥n de **seguimiento de jugadores (tracking)** en jugadas seleccionadas.\n- Construir un modelo que **prediga una variable de salida continua**, relacionada con el desempe√±o ofensivo (por ejemplo, yardas ganadas, probabilidad de conversi√≥n o m√©tricas derivadas).\n- Entrenar, validar y evaluar modelos de regresi√≥n capaces de generalizar a jugadas no vistas.\n\nEl **score oficial** de la competencia se basa en una m√©trica de error (por ejemplo, MSE o MAE) que mide la **precisi√≥n de la predicci√≥n** frente al resultado real de la jugada.\n\n---\n## 2. Descripci√≥n de variables de entrada y salida\n\n| **Variable** | **Tipo de Dato** | **Descripci√≥n** | **Rol en el Modelo** |\n|---------------|------------------|------------------|-----------------------|\n| `game_id` | Num√©rica | Identificador √∫nico del juego. | Identificaci√≥n / Uni√≥n |\n| `play_id` | Num√©rica | Identificador de la jugada (no √∫nico entre juegos). | Identificaci√≥n / Uni√≥n |\n| `player_to_predict` | Booleana | Indica si se deben predecir las coordenadas (x, y) de este jugador. | Indicador de objetivo |\n| `nfl_id` | Num√©rica | Identificador √∫nico del jugador en la NFL. | Identificaci√≥n |\n| `frame_id` | Num√©rica | N√∫mero de frame dentro de la jugada. Inicia en 1. | Variable temporal |\n| `play_direction` | Categ√≥rica | Direcci√≥n del ataque: izquierda o derecha. | Contextual / Categ√≥rica |\n| `absolute_yardline_number` | Num√©rica | Distancia desde la zona de anotaci√≥n del equipo en posesi√≥n (yardas). | Posicional |\n| `player_name` | Texto | Nombre del jugador. | Informativa (no predictiva directa) |\n| `player_height` | Texto / Num√©rica | Altura del jugador en pies y pulgadas (ft-in). | Fisiol√≥gica |\n| `player_weight` | Num√©rica | Peso del jugador en libras (lbs). | Fisiol√≥gica |\n| `player_birth_date` | Fecha | Fecha de nacimiento del jugador (yyyy-mm-dd). | Demogr√°fica |\n| `player_position` | Categ√≥rica | Posici√≥n t√≠pica en el campo (QB, WR, CB, etc.). | Categ√≥rica |\n| `player_side` | Categ√≥rica | Lado del equipo: Ofensivo o Defensivo. | Categ√≥rica |\n| `player_role` | Categ√≥rica | Rol del jugador en la jugada (Passer, Receiver, Defensive Coverage, etc.). | Categ√≥rica / Contextual |\n| `x` | Num√©rica | Posici√≥n del jugador sobre el eje longitudinal del campo (0 ‚Äì 120 yardas). | Posici√≥n espacial |\n| `y` | Num√©rica | Posici√≥n del jugador sobre el eje transversal del campo (0 ‚Äì 53.3 yardas). | Posici√≥n espacial |\n| `s` | Num√©rica | Velocidad del jugador (yardas/segundo). | Cin√©tica |\n| `a` | Num√©rica | Aceleraci√≥n del jugador (yardas/segundo¬≤). | Cin√©tica |\n| `o` | Num√©rica | Orientaci√≥n del jugador en grados (hacia d√≥nde est√° mirando). | Angular |\n| `dir` | Num√©rica | Direcci√≥n de movimiento en grados (hacia d√≥nde se desplaza). | Angular |\n| `num_frames_output` | Num√©rica | N√∫mero de frames futuros a predecir para esa jugada/jugador. | Temporal / Objetivo de predicci√≥n |\n| `ball_land_x` | Num√©rica | Coordenada X donde se espera que aterrice el bal√≥n. | Contextual / Futura |\n| `ball_land_y` | Num√©rica | Coordenada Y donde se espera que aterrice el bal√≥n. | Contextual / Futura |\n| `x` *(en output)* | Num√©rica | Posici√≥n **real futura** del jugador en el eje longitudinal. | **Variable de salida (target)** |\n| `y` *(en output)* | Num√©rica | Posici√≥n **real futura** del jugador en el eje transversal. | **Variable de salida (target)** |\n\n---\n\n## 3. An√°lisis Exploratorio de Datos (EDA)\nEl an√°lisis exploratorio se realiza para comprender la estructura y comportamiento de las variables antes de construir modelos predictivos.  \nA continuaci√≥n, se describen los principales pasos aplicados sobre el conjunto de datos de seguimiento de jugadores (tracking data) de la NFL:","metadata":{}},{"cell_type":"markdown","source":"### Inspecci√≥n general de la base de datos","metadata":{}},{"cell_type":"code","source":"train_df.info #Muestra informaci√≥n general del DataFrame","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:18:25.793121Z","iopub.execute_input":"2025-10-22T03:18:25.793446Z","iopub.status.idle":"2025-10-22T03:18:27.453208Z","shell.execute_reply.started":"2025-10-22T03:18:25.793423Z","shell.execute_reply":"2025-10-22T03:18:27.452421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.describe() #Muestra estad√≠sticas descriptivas b√°sicas de las columnas num√©ricas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:18:27.454108Z","iopub.execute_input":"2025-10-22T03:18:27.454440Z","iopub.status.idle":"2025-10-22T03:18:29.717175Z","shell.execute_reply.started":"2025-10-22T03:18:27.454414Z","shell.execute_reply":"2025-10-22T03:18:29.716360Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.head() #Muestra las primeras filas del dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:18:29.718192Z","iopub.execute_input":"2025-10-22T03:18:29.718518Z","iopub.status.idle":"2025-10-22T03:18:29.737442Z","shell.execute_reply.started":"2025-10-22T03:18:29.718490Z","shell.execute_reply":"2025-10-22T03:18:29.736674Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Conteo de jugadores, jugadas y partidos √∫nicos","metadata":{}},{"cell_type":"code","source":"train_df[['game_id', 'play_id', 'nfl_id']].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:18:29.738489Z","iopub.execute_input":"2025-10-22T03:18:29.738822Z","iopub.status.idle":"2025-10-22T03:18:29.851832Z","shell.execute_reply.started":"2025-10-22T03:18:29.738794Z","shell.execute_reply":"2025-10-22T03:18:29.851291Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### An√°lisis de variables numericas","metadata":{}},{"cell_type":"code","source":"train_df.hist(bins=50, figsize=(20,15))\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:18:29.852568Z","iopub.execute_input":"2025-10-22T03:18:29.852837Z","iopub.status.idle":"2025-10-22T03:18:35.070655Z","shell.execute_reply.started":"2025-10-22T03:18:29.852811Z","shell.execute_reply":"2025-10-22T03:18:35.069840Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Correlaci√≥n variables n√∫mericas","metadata":{}},{"cell_type":"code","source":"corr_matrix = train_df.corr(numeric_only=True)\ncorr_matrix.style.background_gradient(cmap='coolwarm')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:18:35.071439Z","iopub.execute_input":"2025-10-22T03:18:35.071656Z","iopub.status.idle":"2025-10-22T03:18:38.605156Z","shell.execute_reply.started":"2025-10-22T03:18:35.071639Z","shell.execute_reply":"2025-10-22T03:18:38.604414Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### An√°lisis de variables categ√≥ricas","metadata":{}},{"cell_type":"code","source":"cat_cols = ['player_position', 'player_side', 'player_role', 'play_direction']\nfor col in cat_cols:\n    plt.figure(figsize=(8,4))\n    sns.countplot(data=train_df, x=col)\n    plt.title(f\"Distribuci√≥n de {col}\")\n    plt.xticks(rotation=45)\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:18:38.607342Z","iopub.execute_input":"2025-10-22T03:18:38.607925Z","iopub.status.idle":"2025-10-22T03:18:47.254131Z","shell.execute_reply.started":"2025-10-22T03:18:38.607904Z","shell.execute_reply":"2025-10-22T03:18:47.253164Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### An√°lisis espacial","metadata":{}},{"cell_type":"code","source":"sample_play = train_df[(train_df['game_id']==train_df['game_id'].iloc[0]) & (train_df['play_id']==train_df['play_id'].iloc[0])]\nsns.scatterplot(data=sample_play, x='x', y='y', hue='player_role')\nplt.title(\"Posiciones de jugadores en una jugada\")\nplt.gca().invert_yaxis()  # Para coincidir con orientaci√≥n real del campo\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:18:47.255189Z","iopub.execute_input":"2025-10-22T03:18:47.255473Z","iopub.status.idle":"2025-10-22T03:18:47.514616Z","shell.execute_reply.started":"2025-10-22T03:18:47.255452Z","shell.execute_reply":"2025-10-22T03:18:47.513860Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Tratamiento de valores faltantes y codificaci√≥n de categ√≥ricas","metadata":{}},{"cell_type":"code","source":"train_df.isnull().sum() #Valores faltantes, en nuestro caso no tenemos ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:18:47.515410Z","iopub.execute_input":"2025-10-22T03:18:47.515703Z","iopub.status.idle":"2025-10-22T03:18:49.054070Z","shell.execute_reply.started":"2025-10-22T03:18:47.515679Z","shell.execute_reply":"2025-10-22T03:18:49.053082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Se implement√≥ un esquema mixto de codificaci√≥n de variables categ√≥ricas:\n\nLabel Encoding en player_name, por su alta cardinalidad (miles de jugadores √∫nicos).\n\nOne-Hot Encoding en player_position, player_side, player_role y play_direction, dado que presentan un n√∫mero reducido de categor√≠as.\nEste enfoque equilibra la interpretabilidad y la eficiencia computacional, evitando la explosi√≥n dimensional asociada al One-Hot Encoding.","metadata":{}},{"cell_type":"code","source":" #Hacemos una copia de train_df en train_df_encoded debido a que vamos a codificar las variables categoricas y no queremos alterar la base original\ntrain_df_encoded = train_df.copy()\n\n# ===== Label Encoding =====\nlabel_vars = ['player_name']\nle = LabelEncoder()\n\nfor col in label_vars:\n    train_df_encoded[col + '_encoded'] = le.fit_transform(train_df_encoded[col])\ntrain_df_encoded['player_name'] = le.fit_transform(train_df_encoded['player_name'])\n\n# ===== One-Hot Encoding =====\nonehot_vars = ['player_position', 'player_side', 'player_role', 'play_direction']\ntrain_df_encoded = pd.get_dummies(train_df_encoded, columns=onehot_vars, drop_first=True)\n\n# Revisar resultado\ntrain_df_encoded.info()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:18:49.055097Z","iopub.execute_input":"2025-10-22T03:18:49.055376Z","iopub.status.idle":"2025-10-22T03:18:55.014928Z","shell.execute_reply.started":"2025-10-22T03:18:49.055355Z","shell.execute_reply":"2025-10-22T03:18:55.014105Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Ingenier√≠a de caracter√≠sticas (Feature Engineering)\nEsto es **crear nuevas variables** a partir de las **existentes**.\n\nPara nuestro modelo vamos a utilizar la **edad del jugador**, sacandola de su **fecha de nacimiento** y vamos a **convertir la estatua a metros**","metadata":{}},{"cell_type":"code","source":"# === Convertir fecha de nacimiento a datetime ===\ntrain_df_encoded['player_birth_date'] = pd.to_datetime(train_df_encoded['player_birth_date'], errors='coerce')\n\n# === Calcular edad (en a√±os) ===\ntoday = pd.Timestamp(datetime.today())\ntrain_df_encoded['player_age'] = (today - train_df_encoded['player_birth_date']).dt.days / 365.25\n\n# === Convertir estatura (ft-in) a metros ===\n# Ejemplo de formato: \"6-2\" (6 pies y 2 pulgadas)\ndef height_to_meters(height_str):\n    try:\n        feet, inches = height_str.split('-')\n        total_inches = int(feet) * 12 + int(inches)\n        meters = total_inches * 0.0254  # 1 pulgada = 0.0254 m\n        return meters\n    except:\n        return None  # Para valores nulos o mal formateados\n\ntrain_df_encoded['player_height_m'] = train_df_encoded['player_height'].apply(height_to_meters)\n\n# === Eliminar columnas originales no num√©ricas ===\ntrain_df_encoded.drop(columns=['player_birth_date', 'player_height'], inplace=True)\n\n# === Verificar resultado ===\ntrain_df_encoded.info()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:18:55.015837Z","iopub.execute_input":"2025-10-22T03:18:55.016225Z","iopub.status.idle":"2025-10-22T03:18:58.481530Z","shell.execute_reply.started":"2025-10-22T03:18:55.016186Z","shell.execute_reply":"2025-10-22T03:18:58.480774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr = train_df_encoded.corr(numeric_only=True)\nplt.figure(figsize=(10,8))\nsns.heatmap(corr, cmap='coolwarm')\nplt.title('Matriz de correlaci√≥n')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:18:58.482453Z","iopub.execute_input":"2025-10-22T03:18:58.483096Z","iopub.status.idle":"2025-10-22T03:19:21.278147Z","shell.execute_reply.started":"2025-10-22T03:18:58.483065Z","shell.execute_reply":"2025-10-22T03:19:21.277417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Parte dos:\n### - Crear los conjuntos de entrenamiento, validaci√≥n y prueba (60/20/20). \n### - Implementar y comparar los modelos de regresi√≥n seleccionados. \n### - Aplicar optimizaci√≥n bayesiana para ajustar hiperpar√°metros. \n### - Calcular y analizar las m√©tricas de desempe√±o (MAE, MSE, R¬≤, MAPE). \n### - Generar conclusiones basadas en el rendimiento promedio y desviaci√≥n est√°ndar de cada modelo.","metadata":{}},{"cell_type":"markdown","source":"### 1. Crear los conjuntos de entrenamiento, validaci√≥n y prueba (60/20/20).","metadata":{}},{"cell_type":"code","source":"# ==========================================\n# üîπ Conversi√≥n segura de todos los sets a float32\n# Compatible con pandas, cuDF y numpy\n# ==========================================\nimport numpy as np\nimport pandas as pd\nimport gc\n\ndef to_float32_safe(df):\n    \"\"\"Convierte cualquier DataFrame, Series o array a float32 de forma segura.\"\"\"\n    if df is None:\n        return None\n    if isinstance(df, (pd.DataFrame, pd.Series)):\n        return df.astype(np.float32)\n    elif hasattr(df, \"astype\"):  # Para cuDF o numpy\n        return df.astype(np.float32)\n    else:\n        return df  # No se puede convertir\n\n# Lista de variables esperadas\ndatasets = [\"X_train\", \"X_val\", \"X_test\", \"y_train\", \"y_val\", \"y_test\"]\n\nfor name in datasets:\n    if name in locals():\n        print(f\"üßÆ Convirtiendo {name} a float32...\")\n        locals()[name] = to_float32_safe(locals()[name])\n\ngc.collect()  # Limpieza de memoria\n\n# Mostrar resumen de dtypes de X_train si existe\nif \"X_train\" in locals():\n    print(\"\\n‚úÖ Conversi√≥n a float32 completada.\")\n    print(\"üìä Resumen de tipos de X_train:\")\n    try:\n        print(locals()[\"X_train\"].dtypes.value_counts())\n    except Exception:\n        print(\"X_train no tiene atributo dtypes (posiblemente es un array o cuDF).\")\nelse:\n    print(\"‚ö†Ô∏è No se encontr√≥ X_train. Verifica tus variables.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:19:21.278949Z","iopub.execute_input":"2025-10-22T03:19:21.279209Z","iopub.status.idle":"2025-10-22T03:19:21.565830Z","shell.execute_reply.started":"2025-10-22T03:19:21.279189Z","shell.execute_reply":"2025-10-22T03:19:21.565217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\n# --- Supongamos que ya tienes tu dataframe procesado ---\n# train_df_encoded es el dataset final tras el preprocesamiento y codificaci√≥n\ndf = train_df_encoded.copy()\n\n# Verificamos que existe un identificador de jugada o game_play\nid_col = 'game_id'  # Use 'game_id' or a combination of 'game_id' and 'play_id' as a unique identifier\n\n# Create a unique play identifier\ndf['game_play_id'] = df['game_id'].astype(str) + '_' + df['play_id'].astype(str)\nid_col = 'game_play_id'\n\n\n# 1Ô∏è‚É£ Primero dividimos a nivel de jugada (no de fila)\nunique_plays = df[id_col].unique()\ntrain_plays, temp_plays = train_test_split(unique_plays, test_size=0.4, random_state=42)\nval_plays, test_plays = train_test_split(temp_plays, test_size=0.5, random_state=42)\n\n# 2Ô∏è‚É£ Creamos las divisiones del dataframe original\ntrain_df = df[df[id_col].isin(train_plays)].reset_index(drop=True)\nval_df   = df[df[id_col].isin(val_plays)].reset_index(drop=True)\ntest_df  = df[df[id_col].isin(test_plays)].reset_index(drop=True)\n\n# 3Ô∏è‚É£ Mostramos tama√±os y proporciones\nprint(\"üîπ Tama√±os del hold-out:\")\nprint(f\"Train: {len(train_df):,} filas ({len(train_df)/len(df):.1%})\")\nprint(f\"Validaci√≥n: {len(val_df):,} filas ({len(val_df)/len(df):.1%})\")\nprint(f\"Test: {len(test_df):,} filas ({len(test_df)/len(df):.1%})\")\n\n# 4Ô∏è‚É£ Si ya tienes variable objetivo (y), la separamos\ntarget_col = 'ball_land_x'  # Change this to the actual name of your dependent variable\ntarget_col_y = 'ball_land_y'\n\n# Check if target columns exist\nif target_col in train_df.columns and target_col_y in train_df.columns:\n    X_train = train_df.drop(columns=[target_col, target_col_y, id_col])\n    y_train = train_df[[target_col, target_col_y]]\n\n    X_val = val_df.drop(columns=[target_col, target_col_y, id_col])\n    y_val = val_df[[target_col, target_col_y]]\n\n    X_test = test_df.drop(columns=[target_col, target_col_y, id_col])\n    y_test = test_df[[target_col, target_col_y]]\n\n    print(\"\\n‚úÖ Divisi√≥n hold-out completada.\")\nelse:\n    print(f\"\\n‚ö†Ô∏è Warning: Target columns '{target_col}' or '{target_col_y}' not found. Skipping target separation.\")\n    X_train = train_df.drop(columns=[id_col])\n    y_train = None # Or handle missing targets as appropriate\n    X_val = val_df.drop(columns=[id_col])\n    y_val = None\n    X_test = test_df.drop(columns=[id_col])\n    y_test = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:19:21.566660Z","iopub.execute_input":"2025-10-22T03:19:21.566915Z","iopub.status.idle":"2025-10-22T03:19:27.989445Z","shell.execute_reply.started":"2025-10-22T03:19:21.566887Z","shell.execute_reply":"2025-10-22T03:19:27.988705Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2. Implementar y comparar los modelos de regresi√≥n seleccionados.","metadata":{}},{"cell_type":"code","source":"!wget -nc https://raw.githubusercontent.com/rapidsai/rapidsai-csp-utils/main/colab/rapids-colab.sh\n!bash rapids-colab.sh stable\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:19:27.990191Z","iopub.execute_input":"2025-10-22T03:19:27.990506Z","iopub.status.idle":"2025-10-22T03:19:28.352119Z","shell.execute_reply.started":"2025-10-22T03:19:27.990478Z","shell.execute_reply":"2025-10-22T03:19:28.351295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================\n# BLOQUE 1: Definici√≥n de modelos e hiperpar√°metros a optimizar\n# ============================================================\n\n# Importaci√≥n de librer√≠as necesarias\n# -----------------------------------\n# optuna: para la optimizaci√≥n bayesiana de hiperpar√°metros.\n# numpy, pandas: manejo de datos num√©ricos y estructurados.\n# sklearn: contiene implementaciones de modelos de regresi√≥n cl√°sicos.\n# xgboost: modelo basado en boosting con √°rboles de decisi√≥n.\n# mean_absolute_error: m√©trica de evaluaci√≥n del error absoluto medio.\nimport optuna\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import (\n    LinearRegression, Lasso, ElasticNet, SGDRegressor, BayesianRidge\n)\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, RationalQuadratic, DotProduct\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# ------------------------------------------------------------\n# Funci√≥n: get_model_and_params(model_name, trial)\n# ------------------------------------------------------------\n# Esta funci√≥n recibe el nombre de un modelo y un objeto `trial` de Optuna,\n# y devuelve una instancia del modelo con los hiperpar√°metros sugeridos\n# durante el proceso de optimizaci√≥n bayesiana.\n# El objetivo es centralizar la definici√≥n de todos los modelos y sus espacios\n# de b√∫squeda de hiperpar√°metros en una sola funci√≥n.\ndef get_model_and_params(model_name, trial):\n\n    # ============================================================\n    # 1Ô∏è‚É£ Regresi√≥n Lineal\n    # ============================================================\n    if model_name == \"LinearRegression\":\n        # Modelo sin hiperpar√°metros ajustables relevantes\n        model = LinearRegression()\n        return model\n\n    # ============================================================\n    # 2Ô∏è‚É£ LASSO (L1 Regularization)\n    # ============================================================\n    elif model_name == \"Lasso\":\n        # 'alpha' controla la penalizaci√≥n L1 (regularizaci√≥n)\n        alpha = trial.suggest_loguniform(\"alpha\", 1e-6, 10)\n        model = Lasso(alpha=alpha, random_state=42, max_iter=5000)\n        return model\n\n    # ============================================================\n    # 3Ô∏è‚É£ Elastic Net (Combinaci√≥n L1 + L2)\n    # ============================================================\n    elif model_name == \"ElasticNet\":\n        alpha = trial.suggest_loguniform(\"alpha\", 1e-6, 10)  # regularizaci√≥n total\n        l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0)  # mezcla entre L1 y L2\n        model = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42, max_iter=5000)\n        return model\n\n    # ============================================================\n    # 4Ô∏è‚É£ Kernel Ridge Regression\n    # ============================================================\n    elif model_name == \"KernelRidge\":\n        alpha = trial.suggest_loguniform(\"alpha\", 1e-3, 10)  # regularizaci√≥n\n        kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"poly\"])  # tipo de n√∫cleo\n        gamma = trial.suggest_loguniform(\"gamma\", 1e-4, 1)  # par√°metro del kernel\n        model = KernelRidge(alpha=alpha, kernel=kernel, gamma=gamma)\n        return model\n\n    # ============================================================\n    # 5Ô∏è‚É£ Stochastic Gradient Descent Regressor\n    # ============================================================\n    elif model_name == \"SGDRegressor\":\n        alpha = trial.suggest_loguniform(\"alpha\", 1e-6, 1e-1)  # regularizaci√≥n\n        penalty = trial.suggest_categorical(\"penalty\", [\"l2\", \"l1\", \"elasticnet\"])  # tipo de penalizaci√≥n\n        eta0 = trial.suggest_loguniform(\"eta0\", 1e-4, 1e-1)  # tasa de aprendizaje inicial\n        learning_rate = trial.suggest_categorical(\"learning_rate\", [\"constant\", \"invscaling\", \"adaptive\"])\n        model = SGDRegressor(alpha=alpha, penalty=penalty, eta0=eta0,\n                             learning_rate=learning_rate, random_state=42, max_iter=2000)\n        return model\n\n    # ============================================================\n    # 6Ô∏è‚É£ Bayesian Ridge Regression\n    # ============================================================\n    elif model_name == \"BayesianRidge\":\n        # Los hiperpar√°metros controlan las distribuciones previas de los coeficientes\n        alpha_1 = trial.suggest_loguniform(\"alpha_1\", 1e-8, 1)\n        alpha_2 = trial.suggest_loguniform(\"alpha_2\", 1e-8, 1)\n        lambda_1 = trial.suggest_loguniform(\"lambda_1\", 1e-8, 1)\n        lambda_2 = trial.suggest_loguniform(\"lambda_2\", 1e-8, 1)\n        model = BayesianRidge(alpha_1=alpha_1, alpha_2=alpha_2,\n                              lambda_1=lambda_1, lambda_2=lambda_2)\n        return model\n\n    # ============================================================\n    # 7Ô∏è‚É£ Gaussian Process Regressor\n    # ============================================================\n    elif model_name == \"GaussianProcessRegressor\":\n        alpha = trial.suggest_loguniform(\"alpha\", 1e-10, 1e-2)  # nivel de ruido del modelo\n        kernel_choice = trial.suggest_categorical(\"kernel\", [\"RBF\", \"DotProduct\", \"RationalQuadratic\"])\n        # Selecci√≥n del kernel (funci√≥n de covarianza)\n        if kernel_choice == \"RBF\":\n            kernel = RBF()\n        elif kernel_choice == \"DotProduct\":\n            kernel = DotProduct()\n        else:\n            kernel = RationalQuadratic()\n        model = GaussianProcessRegressor(alpha=alpha, kernel=kernel, random_state=42)\n        return model\n\n    # ============================================================\n    # 8Ô∏è‚É£ Support Vector Regressor (SVR)\n    # ============================================================\n    elif model_name == \"SVR\":\n        C = trial.suggest_float(\"C\", 1, 1000)  # penalizaci√≥n al error\n        epsilon = trial.suggest_loguniform(\"epsilon\", 1e-3, 0.5)  # zona de tolerancia sin penalizaci√≥n\n        kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\"])  # tipo de kernel\n        gamma = trial.suggest_loguniform(\"gamma\", 1e-4, 1)  # par√°metro del kernel\n        model = SVR(C=C, epsilon=epsilon, kernel=kernel, gamma=gamma)\n        return model\n\n    # ============================================================\n    # 9Ô∏è‚É£ Random Forest Regressor\n    # ============================================================\n    elif model_name == \"RandomForestRegressor\":\n        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)  # n√∫mero de √°rboles\n        max_depth = trial.suggest_int(\"max_depth\", 3, 30)  # profundidad m√°xima\n        max_features = trial.suggest_float(\"max_features\", 0.3, 1.0)  # proporci√≥n de variables por √°rbol\n        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)  # tama√±o m√≠nimo de hojas\n        model = RandomForestRegressor(\n            n_estimators=n_estimators, max_depth=max_depth,\n            max_features=max_features, min_samples_leaf=min_samples_leaf,\n            random_state=42, n_jobs=-1)\n        return model\n\n    # ============================================================\n    # üîü Gradient Boosting Regressor\n    # ============================================================\n    elif model_name == \"GradientBoostingRegressor\":\n        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3)\n        max_depth = trial.suggest_int(\"max_depth\", 2, 10)\n        subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)  # fracci√≥n de muestras por iteraci√≥n\n        model = GradientBoostingRegressor(\n            n_estimators=n_estimators, learning_rate=learning_rate,\n            max_depth=max_depth, subsample=subsample, random_state=42)\n        return model\n\n    # ============================================================\n    # 1Ô∏è‚É£1Ô∏è‚É£ XGBoost Regressor\n    # ============================================================\n    elif model_name == \"XGBoost\":\n        n_estimators = trial.suggest_int(\"n_estimators\", 100, 1000)\n        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3)\n        max_depth = trial.suggest_int(\"max_depth\", 3, 10)\n        subsample = trial.suggest_float(\"subsample\", 0.5, 1.0)\n        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0)\n        reg_lambda = trial.suggest_loguniform(\"lambda\", 1e-6, 100)  # regularizaci√≥n L2\n        reg_alpha = trial.suggest_loguniform(\"alpha\", 1e-6, 100)   # regularizaci√≥n L1\n        model = XGBRegressor(\n            n_estimators=n_estimators, learning_rate=learning_rate,\n            max_depth=max_depth, subsample=subsample, colsample_bytree=colsample_bytree,\n            reg_lambda=reg_lambda, reg_alpha=reg_alpha,\n            random_state=42, tree_method=\"hist\", n_jobs=-1)\n        return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:19:28.354073Z","iopub.execute_input":"2025-10-22T03:19:28.354353Z","iopub.status.idle":"2025-10-22T03:19:28.816740Z","shell.execute_reply.started":"2025-10-22T03:19:28.354325Z","shell.execute_reply":"2025-10-22T03:19:28.816134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================\n# ‚öôÔ∏è Configuraci√≥n base para todas las optimizaciones\n# =========================================\n\n# ------------------------------------------------------------\n# Importaci√≥n de librer√≠as\n# ------------------------------------------------------------\nimport optuna  # Biblioteca para la optimizaci√≥n bayesiana de hiperpar√°metros\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport numpy as np  # C√°lculo num√©rico eficiente\nimport time  # Medici√≥n del tiempo de ejecuci√≥n de cada modelo\n\n# ------------------------------------------------------------\n# Importaci√≥n de modelos desde RAPIDS (GPU)\n# ------------------------------------------------------------\n# RAPIDS (cuML) ofrece implementaciones aceleradas por GPU de modelos cl√°sicos,\n# permitiendo reducir dr√°sticamente los tiempos de entrenamiento.\n# Solo deben usarse si el entorno de ejecuci√≥n cuenta con GPU compatible (NVIDIA).\nfrom cuml.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom cuml.svm import SVR\nfrom cuml.ensemble import RandomForestRegressor\n\n# ------------------------------------------------------------\n# Importaci√≥n de modelos CPU (Scikit-learn y XGBoost)\n# ------------------------------------------------------------\n# Estas implementaciones se ejecutan sobre CPU y son la referencia base\n# para comparar rendimiento frente a las versiones GPU.\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n\n# ------------------------------------------------------------\n# Definici√≥n de la m√©trica objetivo\n# ------------------------------------------------------------\n# Se define una funci√≥n auxiliar para calcular el error cuadr√°tico medio (MSE),\n# que mide el promedio de los errores al cuadrado entre las predicciones y los valores reales.\n# Esta m√©trica penaliza m√°s los errores grandes, lo cual es √∫til para modelos de regresi√≥n.\ndef score_mse(y_true, y_pred):\n    return mean_squared_error(y_true, y_pred)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:19:28.817501Z","iopub.execute_input":"2025-10-22T03:19:28.817714Z","iopub.status.idle":"2025-10-22T03:19:35.659213Z","shell.execute_reply.started":"2025-10-22T03:19:28.817696Z","shell.execute_reply":"2025-10-22T03:19:35.658423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================\n# üìÅ Inicializar archivo de guardado\n# =========================================\n\nimport os  # M√≥dulo est√°ndar de Python para operaciones con el sistema de archivos\n\n# Definici√≥n del nombre del archivo donde se almacenar√°n los mejores resultados\nresults_path = \"best_params.txt\"\n\n# ------------------------------------------------------------\n# Si el archivo de resultados ya existe, se elimina\n# ------------------------------------------------------------\n# Esto garantiza que en cada nueva ejecuci√≥n del script, el archivo se reinicie\n# y no acumule registros de ejecuciones anteriores.\nif os.path.exists(results_path):\n    os.remove(results_path)\n\n# ------------------------------------------------------------\n# Funci√≥n: save_best_params(model_name, best_params, best_score)\n# ------------------------------------------------------------\n# Esta funci√≥n tiene como prop√≥sito registrar en un archivo de texto:\n#   - El nombre del modelo\n#   - Su mejor puntuaci√≥n (MSE)\n#   - Los hiperpar√°metros √≥ptimos encontrados durante la optimizaci√≥n\n#\n# De esta manera, se conserva un registro de los resultados obtenidos\n# en cada ejecuci√≥n de Optuna sin necesidad de revisar manualmente cada trial.\ndef save_best_params(model_name, best_params, best_score):\n    \"\"\"Guarda los hiperpar√°metros y resultados en un archivo .txt\"\"\"\n\n    # Abre el archivo en modo 'a' (append), lo que permite agregar informaci√≥n\n    # sin sobrescribir el contenido existente (ideal si se guardan varios modelos).\n    with open(results_path, \"a\") as f:\n        f.write(f\"üîπ Modelo: {model_name}\\n\")\n        f.write(f\"üìä Mejor Score (MSE): {best_score:.6f}\\n\")\n        f.write(f\"‚öôÔ∏è Hiperpar√°metros: {best_params}\\n\")\n        f.write(\"=\"*70 + \"\\n\")\n\n    # Imprime en consola un mensaje confirmando el guardado del modelo\n    print(f\"‚úÖ Par√°metros de {model_name} guardados en {results_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:19:35.660118Z","iopub.execute_input":"2025-10-22T03:19:35.660882Z","iopub.status.idle":"2025-10-22T03:19:35.666487Z","shell.execute_reply.started":"2025-10-22T03:19:35.660851Z","shell.execute_reply":"2025-10-22T03:19:35.665698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport pandas as pd\nimport numpy as np\n\n# ------------------------------------------------------------\n# Funci√≥n: clean_memory()\n# ------------------------------------------------------------\n# Objetivo:\n#   Liberar memoria tanto en CPU como en GPU antes de comenzar un nuevo\n#   entrenamiento o proceso de optimizaci√≥n. Esto es especialmente √∫til\n#   cuando se ejecutan m√∫ltiples modelos en un mismo entorno (como Colab)\n#   y la memoria puede llenarse f√°cilmente.\n#\n# Mecanismo:\n#   1Ô∏è‚É£ Se invoca el recolector de basura de Python para liberar objetos no referenciados.\n#   2Ô∏è‚É£ Si se detecta PyTorch (torch), se limpia la cach√© de memoria GPU.\n#   3Ô∏è‚É£ Si se detecta CuPy (usado por RAPIDS/cuML), se libera la memoria GPU restante.\ndef clean_memory():\n    \"\"\"Libera memoria RAM y GPU antes de entrenar.\"\"\"\n\n    # üß† Paso 1: Limpieza de memoria en CPU\n    gc.collect()  # Fuerza la recolecci√≥n de basura y libera objetos no utilizados\n\n    # ‚öôÔ∏è Paso 2: Limpieza de memoria en GPU si se usa PyTorch\n    try:\n        import torch\n        torch.cuda.empty_cache()  # Libera la memoria reservada en la GPU por PyTorch\n    except:\n        pass  # Si PyTorch no est√° instalado, simplemente contin√∫a\n\n    # ‚öôÔ∏è Paso 3: Limpieza de memoria en GPU si se usa CuPy/RAPIDS\n    try:\n        import cupy as cp\n        cp.get_default_memory_pool().free_all_blocks()  # Libera todos los bloques de memoria GPU de CuPy\n    except:\n        pass  # Si CuPy no est√° disponible, ignora el error\n\n    # üßπ Mensaje de confirmaci√≥n\n    print(\"üßπ Memoria limpiada correctamente.\")\n\n# ------------------------------------------------------------\n# Ejecuci√≥n inicial\n# ------------------------------------------------------------\n# Se recomienda realizar una limpieza de memoria antes de iniciar el proceso\n# de entrenamiento o de optimizaci√≥n, para asegurar que los recursos del sistema\n# est√©n completamente disponibles.\nclean_memory()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:19:35.667333Z","iopub.execute_input":"2025-10-22T03:19:35.667562Z","iopub.status.idle":"2025-10-22T03:19:39.287792Z","shell.execute_reply.started":"2025-10-22T03:19:35.667535Z","shell.execute_reply":"2025-10-22T03:19:39.287160Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üìä Evaluaci√≥n de Modelos de Regresi√≥n\n\nA continuaci√≥n se listan todos los modelos evaluados, marcando con un ‚úÖ los que **funcionaron correctamente** seg√∫n los resultados obtenidos, y con una ‚ùå los que **no funcionaron**.\n\n| Modelo | ¬øFuncion√≥? |\n|:-----------------------------|:-------------:|\n| **LinearRegression** | ‚úÖ |\n| **Lasso** | ‚úÖ |\n| **Ridge** | ‚úÖ |\n| **ElasticNet** | ‚úÖ |\n| **KernelRidge** | ‚ùå |\n| **SGDRegressor** | ‚ùå |\n| **BayesianRidge** | ‚ùå |\n| **GaussianProcessRegressor** | ‚ùå |\n| **SVR (Support Vector Regressor)** | ‚ùå |\n| **RandomForestRegressor** | ‚ùå |\n| **GradientBoosting / XGBoost** | ‚úÖ |\n\n---\n\n‚úÖ **Modelos que funcionaron:**\n- XGBoost (CPU)\n- Linear Regression\n- Lasso\n- Ridge\n- ElasticNet\n\n‚ùå **Modelos que no funcionaron:**\n- Kernel Ridge\n- SGD Regressor\n- Bayesian Ridge\n- Gaussian Process Regressor\n- SVR\n- Random Forest Regressor\n\n","metadata":{}},{"cell_type":"markdown","source":"## Rapids","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# üéØ Optimizaci√≥n bayesiana para Linear Regression\n# ============================================================\n\ndef objective_lr(trial):\n    \"\"\"\n    Funci√≥n objetivo para Optuna (modelo: Linear Regression)\n\n    Esta funci√≥n define el proceso que Optuna repetir√° varias veces (trials)\n    con distintos conjuntos de hiperpar√°metros, buscando minimizar el error (MSE).\n    \"\"\"\n\n    # --------------------------------------------------------\n    # 1Ô∏è‚É£ Definici√≥n del modelo\n    # --------------------------------------------------------\n    # Se crea una instancia de LinearRegression con el hiperpar√°metro 'fit_intercept'\n    # que puede tomar los valores True o False.\n    # Optuna seleccionar√° el valor que d√© mejor resultado.\n    model = LinearRegression(fit_intercept=trial.suggest_categorical(\"fit_intercept\", [True, False]))\n\n    # --------------------------------------------------------\n    # 2Ô∏è‚É£ Entrenamiento del modelo\n    # --------------------------------------------------------\n    # Se entrena el modelo con los datos de entrenamiento previamente definidos.\n    model.fit(X_train, y_train)\n\n    # --------------------------------------------------------\n    # 3Ô∏è‚É£ Predicciones y evaluaci√≥n\n    # --------------------------------------------------------\n    # Se generan predicciones sobre el conjunto de validaci√≥n.\n    preds = model.predict(X_val)\n\n    # Se calcula el error cuadr√°tico medio (MSE) como m√©trica de desempe√±o.\n    # El objetivo de Optuna ser√° minimizar este valor.\n    return score_mse(y_val, preds)\n\n# ============================================================\n# üßπ Limpieza de memoria antes de comenzar la optimizaci√≥n\n# ============================================================\nclean_memory()\n\n# ============================================================\n# üöÄ Configuraci√≥n del estudio Optuna\n# ============================================================\n# Se crea un nuevo estudio en Optuna, indicando que el objetivo es \"minimizar\" el MSE.\nstudy_lr = optuna.create_study(direction='minimize')\n\n# ------------------------------------------------------------\n# Ejecuci√≥n de la optimizaci√≥n\n# ------------------------------------------------------------\n# - `objective_lr`: funci√≥n que Optuna ejecutar√° en cada trial.\n# - `n_trials=10`: n√∫mero de combinaciones de hiperpar√°metros a evaluar.\n# - `show_progress_bar=True`: muestra una barra de progreso visual durante la ejecuci√≥n.\nstudy_lr.optimize(objective_lr, n_trials=10, show_progress_bar=True)\n\n# ============================================================\n# üèÅ Resultados finales\n# ============================================================\n# Se imprimen los mejores resultados obtenidos durante la b√∫squeda.\nprint(\"üèÅ Mejor resultado Linear Regression:\", study_lr.best_value)\nprint(\"üîß Par√°metros:\", study_lr.best_params)\n\n# ============================================================\n# üíæ Guardado de resultados\n# ============================================================\n# Los mejores hiperpar√°metros y su score son almacenados en el archivo `best_params.txt`\n# mediante la funci√≥n definida previamente.\nsave_best_params(\"Linear Regression\", study_lr.best_params, study_lr.best_value)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T02:51:10.254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## No funciona","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# üéØ Optimizaci√≥n bayesiana para Linear Regression (con polinomios)\n# ============================================================\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\ndef objective_lr_poly(trial):\n    \"\"\"\n    Funci√≥n objetivo para Optuna (modelo: Linear Regression con caracter√≠sticas polinomiales)\n    \"\"\"\n\n    # --------------------------------------------------------\n    # 1Ô∏è‚É£ Hiperpar√°metros a optimizar\n    # --------------------------------------------------------\n    # - fit_intercept: si se incluye el t√©rmino independiente\n    # - grado del polinomio (1 = lineal, 2 = cuadr√°tico, 3 = c√∫bico)\n    fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n    degree = trial.suggest_int(\"degree\", 1, 3)\n\n    # --------------------------------------------------------\n    # 2Ô∏è‚É£ Definici√≥n del pipeline\n    # --------------------------------------------------------\n    # El pipeline aplica primero la expansi√≥n polinomial y luego la regresi√≥n lineal\n    model = Pipeline([\n        (\"poly\", PolynomialFeatures(degree=degree, include_bias=False)),\n        (\"lr\", LinearRegression(fit_intercept=fit_intercept))\n    ])\n\n    # --------------------------------------------------------\n    # 3Ô∏è‚É£ Entrenamiento\n    # --------------------------------------------------------\n    model.fit(X_train, y_train)\n\n    # --------------------------------------------------------\n    # 4Ô∏è‚É£ Predicci√≥n y evaluaci√≥n\n    # --------------------------------------------------------\n    preds = model.predict(X_val)\n    return score_mse(y_val, preds)\n\n# ============================================================\n# üßπ Limpieza de memoria antes de comenzar la optimizaci√≥n\n# ============================================================\nclean_memory()\n\n# ============================================================\n# üöÄ Ejecuci√≥n del estudio Optuna\n# ============================================================\nstudy_lr_poly = optuna.create_study(direction='minimize')\nstudy_lr_poly.optimize(objective_lr_poly, n_trials=15, show_progress_bar=True)\n\n# ============================================================\n# üèÅ Resultados finales\n# ============================================================\nprint(\"üèÅ Mejor resultado (Linear Regression + Polinomio):\", study_lr_poly.best_value)\nprint(\"üîß Par√°metros:\", study_lr_poly.best_params)\n\n# ============================================================\n# üíæ Guardado de resultados\n# ============================================================\nsave_best_params(\"Linear Regression + Polynomial\", study_lr_poly.best_params, study_lr_poly.best_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T02:57:39.031618Z","iopub.execute_input":"2025-10-22T02:57:39.032378Z","execution_failed":"2025-10-22T02:58:06.703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_memory()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T02:00:21.790Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================\n# üß† OptimizacioÃÅn Bayesiana para modelo Lasso con multi-output\n# =========================================\n\n# Importaci√≥n de librer√≠as necesarias\nimport cudf                 # Librer√≠a de RAPIDS para manejar DataFrames en GPU\nimport numpy as np          # Librer√≠a para operaciones num√©ricas\nimport optuna               # Librer√≠a para optimizaci√≥n bayesiana de hiperpar√°metros\nfrom cuml.linear_model import Lasso   # Implementaci√≥n GPU del modelo Lasso (regresi√≥n lineal regularizada)\nfrom sklearn.metrics import mean_squared_error  # M√©trica para calcular error cuadr√°tico medio (MSE)\n\n# ============================================================\n# üéØ Funci√≥n de puntuaci√≥n personalizada: soporte para multi-output\n# ============================================================\ndef score_mse_multioutput(y_true, y_pred):\n    \"\"\"\n    Calcula el MSE promedio cuando existen m√∫ltiples variables objetivo (multi-output).\n    Maneja casos donde las predicciones y valores reales pueden tener diferentes dimensiones.\n    \"\"\"\n\n    # Asegura que los datos sean arrays de numpy\n    if isinstance(y_true, (pd.DataFrame, pd.Series)):\n        y_true = y_true.values\n    if isinstance(y_pred, (pd.DataFrame, pd.Series)):\n        y_pred = y_pred.values\n\n    # Validaci√≥n de dimensiones entre y_true e y_pred\n    if y_true.shape != y_pred.shape:\n        # Si las dimensiones no coinciden, se revisan los posibles motivos\n\n        # Caso: la predicci√≥n tiene una sola dimensi√≥n, pero y_true tiene m√°s\n        if y_true.ndim == 2 and y_pred.ndim == 1:\n            raise ValueError(\"La predicci√≥n es unidimensional, pero los valores verdaderos son bidimensionales. Verifique que el modelo prediga ambas salidas.\")\n\n        # Caso: el n√∫mero de muestras no coincide\n        if y_true.shape[0] != y_pred.shape[0]:\n            raise ValueError(f\"Desajuste en el n√∫mero de muestras: y_true tiene {y_true.shape[0]}, y_pred tiene {y_pred.shape[0]}\")\n\n        # Caso: misma cantidad de filas pero diferente n√∫mero de columnas (dimensi√≥n de salida)\n        if y_true.ndim == 2 and y_pred.ndim == 2 and y_true.shape[1] != y_pred.shape[1]:\n            print(f\"‚ö†Ô∏è Advertencia: desajuste en dimensiones: y_true {y_true.shape}, y_pred {y_pred.shape}. Se intentar√° continuar si es posible.\")\n\n            # Si las predicciones tienen una sola salida cuando deber√≠an ser dos\n            if y_true.shape[1] == 2 and y_pred.shape[1] == 1:\n                raise ValueError(\"El modelo predice solo una salida, pero se requieren dos (X e Y).\")\n\n            # Si tienen el mismo n√∫mero de columnas, contin√∫a sin error\n            elif y_true.shape[1] == y_pred.shape[1]:\n                pass\n            else:\n                raise ValueError(f\"Desajuste irreconciliable entre y_true {y_true.shape} y y_pred {y_pred.shape}\")\n\n    # Caso: salida √∫nica\n    if y_true.ndim == 1 or y_true.shape[1] == 1:\n        return mean_squared_error(y_true, y_pred)\n\n    # Caso: m√∫ltiples salidas ‚Üí se calcula el MSE por cada salida y se promedia\n    else:\n        mse_x = mean_squared_error(y_true[:, 0], y_pred[:, 0])\n        mse_y = mean_squared_error(y_true[:, 1], y_pred[:, 1])\n        return (mse_x + mse_y) / 2.0\n\n\n# ============================================================\n# üß© Funci√≥n objetivo para Optuna (entrenamiento + evaluaci√≥n)\n# ============================================================\ndef objective_lasso(trial):\n    \"\"\"\n    Funci√≥n objetivo para la optimizaci√≥n bayesiana con Optuna.\n    Entrena dos modelos Lasso independientes (uno por cada variable objetivo)\n    y devuelve el error cuadr√°tico medio promedio (MSE).\n    \"\"\"\n\n    # Hiperpar√°metro a optimizar ‚Üí fuerza de regularizaci√≥n L1\n    alpha = trial.suggest_float(\"alpha\", 0.0001, 1.0, log=True)\n\n    # Conversi√≥n expl√≠cita de las variables predictoras a num√©ricas\n    # Se fuerzan los valores no num√©ricos a NaN\n    X_train_numeric = X_train.apply(pd.to_numeric, errors='coerce')\n    X_val_numeric = X_val.apply(pd.to_numeric, errors='coerce')\n\n    # Sustituci√≥n de valores faltantes por 0 para evitar errores en el entrenamiento\n    X_train_numeric.fillna(0, inplace=True)\n    X_val_numeric.fillna(0, inplace=True)\n\n    # Conversi√≥n de los DataFrames a cuDF (para procesamiento en GPU)\n    X_train_gpu = cudf.DataFrame.from_pandas(X_train_numeric)\n    X_val_gpu = cudf.DataFrame.from_pandas(X_val_numeric)\n\n    # Conversi√≥n de los valores reales (targets) a NumPy\n    y_train_np = y_train.values if isinstance(y_train, (pd.DataFrame, pd.Series)) else y_train\n    y_val_np = y_val.values if isinstance(y_val, (pd.DataFrame, pd.Series)) else y_val\n\n    # Entrenamiento de dos modelos Lasso independientes: uno para cada salida (X e Y)\n    model_x = Lasso(alpha=alpha, max_iter=1000)\n    model_y = Lasso(alpha=alpha, max_iter=1000)\n\n    # Modelo para la primera salida (X)\n    y_train_x_gpu = cudf.Series(y_train_np[:, 0].astype(np.float32))\n    model_x.fit(X_train_gpu, y_train_x_gpu)\n    preds_x = model_x.predict(X_val_gpu)\n\n    # Modelo para la segunda salida (Y)\n    y_train_y_gpu = cudf.Series(y_train_np[:, 1].astype(np.float32))\n    model_y.fit(X_train_gpu, y_train_y_gpu)\n    preds_y = model_y.predict(X_val_gpu)\n\n    # Se combinan ambas predicciones en un solo array (n_samples, 2)\n    preds_combined = np.stack([preds_x.to_numpy(), preds_y.to_numpy()], axis=1)\n\n    # Se calcula el MSE promedio entre las dos salidas\n    return score_mse_multioutput(y_val_np, preds_combined)\n\n\n# ============================================================\n# üßπ Limpieza de memoria antes de iniciar el estudio\n# ============================================================\nclean_memory()\n\n\n# ============================================================\n# üöÄ Creaci√≥n y ejecuci√≥n del estudio de optimizaci√≥n\n# ============================================================\n\n# Se define el estudio de Optuna, que buscar√° minimizar el MSE\nstudy_lasso = optuna.create_study(direction='minimize')\n\n# Se ejecuta la optimizaci√≥n durante 20 iteraciones (trials)\nstudy_lasso.optimize(objective_lasso, n_trials=20, show_progress_bar=True)\n\n# ============================================================\n# üìä Resultados finales del modelo\n# ============================================================\nprint(\"üèÅ Mejor resultado Lasso:\", study_lasso.best_value)\nprint(\"üîß Par√°metros √≥ptimos:\", study_lasso.best_params)\n\n# Guardado de los mejores par√°metros en archivo externo\nsave_best_params(\"Lasso\", study_lasso.best_params, study_lasso.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T02:13:32.489918Z","iopub.execute_input":"2025-10-22T02:13:32.490314Z","iopub.status.idle":"2025-10-22T02:14:11.363741Z","shell.execute_reply.started":"2025-10-22T02:13:32.490279Z","shell.execute_reply":"2025-10-22T02:14:11.362883Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## No funciona","metadata":{}},{"cell_type":"code","source":"# ============================================================\n# üéØ Optuna + cuML Lasso con caracter√≠sticas polinomiales\n# ============================================================\n\nfrom sklearn.preprocessing import PolynomialFeatures\nimport cudf\nimport numpy as np\nimport optuna\nfrom cuml.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error\n\ndef objective_lasso_poly(trial):\n    \"\"\"\n    Optimizaci√≥n de Lasso (GPU) con caracter√≠sticas polinomiales.\n    \"\"\"\n\n    # --------------------------------------------------------\n    # 1Ô∏è‚É£ Hiperpar√°metros\n    # --------------------------------------------------------\n    alpha = trial.suggest_float(\"alpha\", 0.0001, 1.0, log=True)\n    degree = trial.suggest_int(\"degree\", 1, 3)  # polinomio de grado 1 a 3\n\n    # --------------------------------------------------------\n    # 2Ô∏è‚É£ Generaci√≥n de caracter√≠sticas polinomiales (en CPU)\n    # --------------------------------------------------------\n    poly = PolynomialFeatures(degree=degree, include_bias=False)\n    X_train_poly = poly.fit_transform(X_train)\n    X_val_poly = poly.transform(X_val)\n\n    # --------------------------------------------------------\n    # 3Ô∏è‚É£ Conversi√≥n a cuDF (GPU)\n    # --------------------------------------------------------\n    X_train_gpu = cudf.DataFrame(X_train_poly.astype(np.float32))\n    X_val_gpu = cudf.DataFrame(X_val_poly.astype(np.float32))\n\n    y_train_np = y_train.values if isinstance(y_train, (pd.DataFrame, pd.Series)) else y_train\n    y_val_np = y_val.values if isinstance(y_val, (pd.DataFrame, pd.Series)) else y_val\n\n    # --------------------------------------------------------\n    # 4Ô∏è‚É£ Entrenamiento de los dos modelos (multi-output)\n    # --------------------------------------------------------\n    model_x = Lasso(alpha=alpha, max_iter=1000)\n    model_y = Lasso(alpha=alpha, max_iter=1000)\n\n    model_x.fit(X_train_gpu, cudf.Series(y_train_np[:, 0].astype(np.float32)))\n    model_y.fit(X_train_gpu, cudf.Series(y_train_np[:, 1].astype(np.float32)))\n\n    preds_x = model_x.predict(X_val_gpu).to_numpy()\n    preds_y = model_y.predict(X_val_gpu).to_numpy()\n\n    preds_combined = np.stack([preds_x, preds_y], axis=1)\n\n    # --------------------------------------------------------\n    # 5Ô∏è‚É£ Evaluaci√≥n (MSE promedio)\n    # --------------------------------------------------------\n    mse_x = mean_squared_error(y_val_np[:, 0], preds_x)\n    mse_y = mean_squared_error(y_val_np[:, 1], preds_y)\n    return (mse_x + mse_y) / 2.0\n\n\n# ============================================================\n# üöÄ Ejecuci√≥n de la optimizaci√≥n\n# ============================================================\nclean_memory()\n\nstudy_lasso_poly = optuna.create_study(direction='minimize')\nstudy_lasso_poly.optimize(objective_lasso_poly, n_trials=20, show_progress_bar=True)\n\nprint(\"üèÅ Mejor resultado (Lasso + Polinomio):\", study_lasso_poly.best_value)\nprint(\"üîß Par√°metros √≥ptimos:\", study_lasso_poly.best_params)\n\nsave_best_params(\"Lasso + Polynomial\", study_lasso_poly.best_params, study_lasso_poly.best_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:08:16.298185Z","iopub.execute_input":"2025-10-22T03:08:16.299108Z","execution_failed":"2025-10-22T03:08:36.860Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_memory()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T02:00:21.790Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ==========================================\n# üöÄ Optimizaci√≥n Bayesiana: Ridge (cuML + GPU)\n# ==========================================\n\n# Importaci√≥n de librer√≠as necesarias\nimport cudf              # Permite manejar DataFrames en GPU mediante RAPIDS\nimport numpy as np       # Librer√≠a para manejo de arreglos num√©ricos\nimport optuna            # Librer√≠a de optimizaci√≥n bayesiana\nfrom cuml.linear_model import Ridge  # Implementaci√≥n GPU del modelo Ridge (regresi√≥n lineal con regularizaci√≥n L2)\n\n\n# ============================================================\n# üß© Funci√≥n objetivo para Optuna (entrenamiento y evaluaci√≥n)\n# ============================================================\ndef objective_ridge(trial):\n    \"\"\"\n    Funci√≥n de optimizaci√≥n que entrena el modelo Ridge con un valor de regularizaci√≥n (alpha)\n    determinado por Optuna y eval√∫a su desempe√±o en t√©rminos de error cuadr√°tico medio (MSE).\n    \"\"\"\n\n    # Hiperpar√°metro a optimizar: el par√°metro de regularizaci√≥n L2\n    alpha = trial.suggest_float(\"alpha\", 1e-4, 10.0, log=True)\n\n    # ------------------------------------------------------------\n    # üßÆ Conversi√≥n de los conjuntos de entrenamiento y validaci√≥n a GPU\n    # ------------------------------------------------------------\n\n    # Si los DataFrames no son de tipo cuDF, se convierten desde pandas\n    X_train_gpu = cudf.DataFrame.from_pandas(X_train) if not isinstance(X_train, cudf.DataFrame) else X_train\n    X_val_gpu   = cudf.DataFrame.from_pandas(X_val) if not isinstance(X_val, cudf.DataFrame) else X_val\n\n    # Se convierten todas las columnas a tipo float32 (necesario para cuML)\n    X_train_gpu = X_train_gpu.astype(np.float32)\n    X_val_gpu   = X_val_gpu.astype(np.float32)\n\n    # ------------------------------------------------------------\n    # üß† Entrenamiento de modelos separados para cada variable objetivo (multi-output)\n    # ------------------------------------------------------------\n\n    # Se crean dos modelos Ridge independientes:\n    # uno para predecir la primera salida (X) y otro para la segunda (Y)\n    model_x = Ridge(alpha=alpha, fit_intercept=True)\n    model_y = Ridge(alpha=alpha, fit_intercept=True)\n\n    # Conversi√≥n de las salidas verdaderas a cuDF.Series de tipo float32\n    y_train_x_gpu = cudf.Series(y_train.iloc[:, 0].astype(np.float32))\n    y_train_y_gpu = cudf.Series(y_train.iloc[:, 1].astype(np.float32))\n\n    # ------------------------------------------------------------\n    # ‚öôÔ∏è Entrenamiento de los modelos en GPU\n    # ------------------------------------------------------------\n\n    # Entrena el modelo para la primera salida\n    model_x.fit(X_train_gpu, y_train_x_gpu)\n    preds_x = model_x.predict(X_val_gpu)   # Predicciones para la primera salida\n\n    # Entrena el modelo para la segunda salida\n    model_y.fit(X_train_gpu, y_train_y_gpu)\n    preds_y = model_y.predict(X_val_gpu)   # Predicciones para la segunda salida\n\n    # ------------------------------------------------------------\n    # üî¢ Combinaci√≥n de predicciones para evaluar el desempe√±o global\n    # ------------------------------------------------------------\n\n    # Se apilan las predicciones en una matriz (n_muestras, 2)\n    preds_combined = np.stack([preds_x.to_numpy(), preds_y.to_numpy()], axis=1)\n\n    # Se convierte y_val a formato numpy para poder calcular el MSE\n    y_val_np = y_val.to_numpy()\n\n    # ------------------------------------------------------------\n    # üéØ Retornar la m√©trica a minimizar (Error Cuadr√°tico Medio Promedio)\n    # ------------------------------------------------------------\n    return score_mse_multioutput(y_val_np, preds_combined)\n\n\n# ============================================================\n# üßπ Limpieza de memoria antes del entrenamiento\n# ============================================================\nclean_memory()\n\n\n# ============================================================\n# üöÄ Creaci√≥n y ejecuci√≥n del estudio de optimizaci√≥n con Optuna\n# ============================================================\n\n# Se define el estudio que buscar√° minimizar el MSE\nstudy_ridge = optuna.create_study(direction='minimize')\n\n# Se ejecuta la optimizaci√≥n durante 20 pruebas (n_trials)\nstudy_ridge.optimize(objective_ridge, n_trials=20, show_progress_bar=True)\n\n\n# ============================================================\n# üìä Resultados finales\n# ============================================================\nprint(\"üèÅ Mejor resultado Ridge:\", study_ridge.best_value)\nprint(\"üîß Par√°metros √≥ptimos:\", study_ridge.best_params)\n\n# Guardar los resultados obtenidos por seguridad en el archivo correspondiente\nsave_best_params(\"Ridge\", study_ridge.best_params, study_ridge.best_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T02:14:59.967377Z","iopub.execute_input":"2025-10-22T02:14:59.967675Z","iopub.status.idle":"2025-10-22T02:15:31.989835Z","shell.execute_reply.started":"2025-10-22T02:14:59.967653Z","shell.execute_reply":"2025-10-22T02:15:31.988983Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# üöÄ Optimizaci√≥n Bayesiana: Ridge (cuML + GPU + Polinomial)\n# ==========================================\n\n# Importaci√≥n de librer√≠as necesarias\nimport cudf              # Permite manejar DataFrames en GPU mediante RAPIDS\nimport cupy as cp        # Arreglos en GPU\nimport numpy as np       # Arreglos en CPU\nimport optuna            # Optimizaci√≥n bayesiana\nfrom cuml.linear_model import Ridge\nfrom cuml.preprocessing import PolynomialFeatures  # Para expansi√≥n polin√≥mica\n\n# ============================================================\n# üß© Funci√≥n objetivo para Optuna (entrenamiento y evaluaci√≥n)\n# ============================================================\ndef objective_ridge_poly(trial):\n    \"\"\"\n    Optimizaci√≥n Bayesiana con Ridge (GPU) y expansi√≥n polin√≥mica de caracter√≠sticas.\n    Entrena modelos independientes para cada salida (multi-output).\n    \"\"\"\n\n    # üîß Hiperpar√°metros a optimizar\n    alpha = trial.suggest_float(\"alpha\", 1e-4, 10.0, log=True)\n    degree = trial.suggest_int(\"degree\", 1, 3)  # Grado del polinomio (1=sin cambio, 2=c√∫adrico, 3=c√∫bico)\n\n    # ------------------------------------------------------------\n    # üßÆ Conversi√≥n de los conjuntos de datos a GPU (cuDF)\n    # ------------------------------------------------------------\n    X_train_gpu = cudf.DataFrame.from_pandas(X_train) if not isinstance(X_train, cudf.DataFrame) else X_train\n    X_val_gpu   = cudf.DataFrame.from_pandas(X_val) if not isinstance(X_val, cudf.DataFrame) else X_val\n\n    X_train_gpu = X_train_gpu.astype(cp.float32)\n    X_val_gpu   = X_val_gpu.astype(cp.float32)\n\n    # ------------------------------------------------------------\n    # üß† Expansi√≥n polin√≥mica de las caracter√≠sticas\n    # ------------------------------------------------------------\n    poly = PolynomialFeatures(degree=degree, include_bias=False)\n    X_train_poly = poly.fit_transform(X_train_gpu)\n    X_val_poly   = poly.transform(X_val_gpu)\n\n    # ------------------------------------------------------------\n    # üéØ Entrenamiento multi-output\n    # ------------------------------------------------------------\n    model_x = Ridge(alpha=alpha, fit_intercept=True)\n    model_y = Ridge(alpha=alpha, fit_intercept=True)\n\n    y_train_x_gpu = cudf.Series(y_train.iloc[:, 0].astype(np.float32))\n    y_train_y_gpu = cudf.Series(y_train.iloc[:, 1].astype(np.float32))\n\n    # Entrenamiento de ambos modelos\n    model_x.fit(X_train_poly, y_train_x_gpu)\n    preds_x = model_x.predict(X_val_poly)\n\n    model_y.fit(X_train_poly, y_train_y_gpu)\n    preds_y = model_y.predict(X_val_poly)\n\n    # ------------------------------------------------------------\n    # üìä Evaluaci√≥n de desempe√±o\n    # ------------------------------------------------------------\n    preds_combined = np.stack([preds_x.to_numpy(), preds_y.to_numpy()], axis=1)\n    y_val_np = y_val.to_numpy()\n\n    return score_mse_multioutput(y_val_np, preds_combined)\n\n# ============================================================\n# üßπ Limpieza antes del entrenamiento\n# ============================================================\nclean_memory()\n\n# ============================================================\n# üöÄ Creaci√≥n y ejecuci√≥n del estudio Optuna\n# ============================================================\nstudy_ridge_poly = optuna.create_study(direction='minimize')\nstudy_ridge_poly.optimize(objective_ridge_poly, n_trials=25, show_progress_bar=True)\n\n# ============================================================\n# üìä Resultados finales\n# ============================================================\nprint(\"üèÅ Mejor resultado Ridge Polin√≥mico:\", study_ridge_poly.best_value)\nprint(\"üîß Par√°metros √≥ptimos:\", study_ridge_poly.best_params)\n\nsave_best_params(\"Ridge_Poly\", study_ridge_poly.best_params, study_ridge_poly.best_value)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# üöÄ Optimizaci√≥n Bayesiana: Ridge (cuML + GPU + Polinomial)\n# ==========================================\n\n# Importaci√≥n de librer√≠as necesarias\nimport cudf              # Permite manejar DataFrames en GPU mediante RAPIDS\nimport cupy as cp        # Arreglos en GPU\nimport numpy as np       # Arreglos en CPU\nimport optuna            # Optimizaci√≥n bayesiana\nfrom cuml.linear_model import Ridge\nfrom cuml.preprocessing import PolynomialFeatures  # Para expansi√≥n polin√≥mica\n\n# ============================================================\n# üß© Funci√≥n objetivo para Optuna (entrenamiento y evaluaci√≥n)\n# ============================================================\ndef objective_ridge_poly(trial):\n    \"\"\"\n    Optimizaci√≥n Bayesiana con Ridge (GPU) y expansi√≥n polin√≥mica de caracter√≠sticas.\n    Entrena modelos independientes para cada salida (multi-output).\n    \"\"\"\n\n    # üîß Hiperpar√°metros a optimizar\n    alpha = trial.suggest_float(\"alpha\", 1e-4, 10.0, log=True)\n    degree = trial.suggest_int(\"degree\", 1, 3)  # Grado del polinomio (1=sin cambio, 2=c√∫adrico, 3=c√∫bico)\n\n    # ------------------------------------------------------------\n    # üßÆ Conversi√≥n de los conjuntos de datos a GPU (cuDF)\n    # ------------------------------------------------------------\n    X_train_gpu = cudf.DataFrame.from_pandas(X_train) if not isinstance(X_train, cudf.DataFrame) else X_train\n    X_val_gpu   = cudf.DataFrame.from_pandas(X_val) if not isinstance(X_val, cudf.DataFrame) else X_val\n\n    X_train_gpu = X_train_gpu.astype(cp.float32)\n    X_val_gpu   = X_val_gpu.astype(cp.float32)\n\n    # ------------------------------------------------------------\n    # üß† Expansi√≥n polin√≥mica de las caracter√≠sticas\n    # ------------------------------------------------------------\n    poly = PolynomialFeatures(degree=degree, include_bias=False)\n    X_train_poly = poly.fit_transform(X_train_gpu)\n    X_val_poly   = poly.transform(X_val_gpu)\n\n    # ------------------------------------------------------------\n    # üéØ Entrenamiento multi-output\n    # ------------------------------------------------------------\n    model_x = Ridge(alpha=alpha, fit_intercept=True)\n    model_y = Ridge(alpha=alpha, fit_intercept=True)\n\n    y_train_x_gpu = cudf.Series(y_train.iloc[:, 0].astype(np.float32))\n    y_train_y_gpu = cudf.Series(y_train.iloc[:, 1].astype(np.float32))\n\n    # Entrenamiento de ambos modelos\n    model_x.fit(X_train_poly, y_train_x_gpu)\n    preds_x = model_x.predict(X_val_poly)\n\n    model_y.fit(X_train_poly, y_train_y_gpu)\n    preds_y = model_y.predict(X_val_poly)\n\n    # ------------------------------------------------------------\n    # üìä Evaluaci√≥n de desempe√±o\n    # ------------------------------------------------------------\n    preds_combined = np.stack([preds_x.to_numpy(), preds_y.to_numpy()], axis=1)\n    y_val_np = y_val.to_numpy()\n\n    return score_mse_multioutput(y_val_np, preds_combined)\n\n# ============================================================\n# üßπ Limpieza antes del entrenamiento\n# ============================================================\nclean_memory()\n\n# ============================================================\n# üöÄ Creaci√≥n y ejecuci√≥n del estudio Optuna\n# ============================================================\nstudy_ridge_poly = optuna.create_study(direction='minimize')\nstudy_ridge_poly.optimize(objective_ridge_poly, n_trials=25, show_progress_bar=True)\n\n# ============================================================\n# üìä Resultados finales\n# ============================================================\nprint(\"üèÅ Mejor resultado Ridge Polin√≥mico:\", study_ridge_poly.best_value)\nprint(\"üîß Par√°metros √≥ptimos:\", study_ridge_poly.best_params)\n\nsave_best_params(\"Ridge_Poly\", study_ridge_poly.best_params, study_ridge_poly.best_value)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_memory()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T02:00:21.790Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================\n# ‚öôÔ∏è Optimizaci√≥n Bayesiana para ElasticNet (cuML + GPU) ‚Äî Versi√≥n corregida\n# ==============================================================\n\nimport optuna\nimport cudf\nimport cupy as cp\nimport numpy as np\nimport pandas as pd\nfrom cuml.linear_model import ElasticNet\n\ndef objective_enet(trial):\n    \"\"\"\n    Funci√≥n objetivo para Optuna que entrena el modelo ElasticNet (versi√≥n GPU de cuML)\n    y busca los mejores hiperpar√°metros para minimizar el error cuadr√°tico medio (MSE).\n    \"\"\"\n\n    # ------------------------------------------------------------\n    # üéØ Definici√≥n de los hiperpar√°metros a optimizar\n    # ------------------------------------------------------------\n    alpha = trial.suggest_float(\"alpha\", 0.0001, 1.0, log=True)\n    l1_ratio = trial.suggest_float(\"l1_ratio\", 0.0, 1.0)\n\n    # ------------------------------------------------------------\n    # üßÆ Conversi√≥n de datos a GPU (cuDF + float32)\n    # ------------------------------------------------------------\n    X_train_gpu = cudf.DataFrame.from_pandas(\n        X_train.apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32)\n    )\n    X_val_gpu = cudf.DataFrame.from_pandas(\n        X_val.apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32)\n    )\n\n    # Variables objetivo ‚Üí NumPy (si vienen como pandas)\n    y_train_np = y_train.values if isinstance(y_train, (pd.DataFrame, pd.Series)) else y_train\n    y_val_np = y_val.values if isinstance(y_val, (pd.DataFrame, pd.Series)) else y_val\n\n    # Convertir a cuDF.Series en GPU\n    y_train_x_gpu = cudf.Series(y_train_np[:, 0].astype(np.float32))\n    y_train_y_gpu = cudf.Series(y_train_np[:, 1].astype(np.float32))\n\n    # ------------------------------------------------------------\n    # ü§ñ Entrenamiento de los modelos (uno por salida)\n    # ------------------------------------------------------------\n    model_x = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=1000)\n    model_y = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, max_iter=1000)\n\n    model_x.fit(X_train_gpu, y_train_x_gpu)\n    model_y.fit(X_train_gpu, y_train_y_gpu)\n\n    preds_x = model_x.predict(X_val_gpu)\n    preds_y = model_y.predict(X_val_gpu)\n\n    # ------------------------------------------------------------\n    # üî¢ Combinar predicciones directamente en GPU (usando CuPy)\n    # ------------------------------------------------------------\n    preds_combined_gpu = cp.stack([preds_x.values, preds_y.values], axis=1)\n\n    # Convertimos solo al final a NumPy (CPU) para calcular m√©tricas\n    preds_combined = cp.asnumpy(preds_combined_gpu)\n    y_val_np = y_val_np.astype(np.float32)\n\n    # ------------------------------------------------------------\n    # üéØ C√°lculo del MSE combinado\n    # ------------------------------------------------------------\n    return score_mse_multioutput(y_val_np, preds_combined)\n\n\n# ==============================================================\n# üöÄ Ejecuci√≥n de la optimizaci√≥n con Optuna\n# ==============================================================\n\nstudy_enet = optuna.create_study(direction='minimize')\nstudy_enet.optimize(objective_enet, n_trials=20, show_progress_bar=True)\n\n# ==============================================================\n# üìä Resultados finales\n# ==============================================================\n\nprint(\"üèÅ Mejor resultado ElasticNet:\", study_enet.best_value)\nprint(\"üîß Par√°metros:\", study_enet.best_params)\n\nsave_best_params(\"ElasticNet\", study_enet.best_params, study_enet.best_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T02:18:48.520867Z","iopub.execute_input":"2025-10-22T02:18:48.521579Z","iopub.status.idle":"2025-10-22T02:19:30.143648Z","shell.execute_reply.started":"2025-10-22T02:18:48.521541Z","shell.execute_reply":"2025-10-22T02:19:30.142731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_memory()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T02:00:21.791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# ‚ö° Optimizaci√≥n Bayesiana: LinearSVR (cuML + GPU)\n# ==========================================\nimport cudf               # Manejo de DataFrames en GPU (similar a pandas, pero para CUDA)\nimport cupy as cp          # Arrays de GPU (similar a numpy, pero en CUDA)\nimport numpy as np\nimport optuna              # Librer√≠a para optimizaci√≥n bayesiana de hiperpar√°metros\nfrom cuml.svm import LinearSVR  # Implementaci√≥n GPU de LinearSVR (cuML)\nimport pandas as pd        # Para manejo inicial de datos en CPU\n\n# ------------------------------------------\n# üßÆ Funci√≥n de evaluaci√≥n: error cuadr√°tico medio multi-output\n# ------------------------------------------\ndef score_mse_multioutput(y_true, y_pred):\n    \"\"\"\n    Calcula el error cuadr√°tico medio (MSE) para problemas con m√∫ltiples salidas (multi-output).\n    Si hay m√°s de una variable objetivo (por ejemplo, coordenadas X e Y),\n    promedia los errores de cada una.\n    \"\"\"\n\n    # Aseguramos que los datos est√©n en formato numpy\n    if isinstance(y_true, (pd.DataFrame, pd.Series)):\n        y_true = y_true.values\n    if isinstance(y_pred, (pd.DataFrame, pd.Series)):\n        y_pred = y_pred.values\n\n    # Validaci√≥n de dimensiones\n    if y_true.shape != y_pred.shape:\n        # Verificaci√≥n de errores comunes de forma\n        if y_true.ndim == 2 and y_pred.ndim == 1:\n            raise ValueError(\"Predicci√≥n 1D pero los valores verdaderos son 2D ‚Äî falta una salida del modelo.\")\n        if y_true.shape[0] != y_pred.shape[0]:\n            raise ValueError(f\"Cantidad de muestras distintas: y_true={y_true.shape[0]}, y_pred={y_pred.shape[0]}\")\n        if y_true.ndim == 2 and y_pred.ndim == 2 and y_true.shape[1] != y_pred.shape[1]:\n            raise ValueError(f\"Dimensiones incompatibles entre y_true {y_true.shape} y y_pred {y_pred.shape}\")\n\n    # Caso de una sola salida\n    if y_true.ndim == 1 or y_true.shape[1] == 1:\n        return mean_squared_error(y_true, y_pred)\n    else:\n        # Caso multi-output: promedio de MSE por variable\n        mse_x = mean_squared_error(y_true[:, 0], y_pred[:, 0])\n        mse_y = mean_squared_error(y_true[:, 1], y_pred[:, 1])\n        return (mse_x + mse_y) / 2.0\n\n\n# ------------------------------------------\n# üß† Funci√≥n objetivo para Optuna\n# ------------------------------------------\ndef objective_linear_svr(trial):\n    \"\"\"\n    Funci√≥n de evaluaci√≥n para Optuna que entrena y eval√∫a un modelo LinearSVR con GPU.\n    Optuna ajusta los hiperpar√°metros 'C' y 'epsilon' para minimizar el MSE.\n    \"\"\"\n\n    # Hiperpar√°metros a optimizar (definidos por b√∫squeda bayesiana)\n    C = trial.suggest_float(\"C\", 0.1, 10.0, log=True)\n    epsilon = trial.suggest_float(\"epsilon\", 0.001, 1.0, log=True)\n\n    # Conversi√≥n de datos a formato cuDF (GPU) y float32\n    X_train_gpu = cudf.DataFrame.from_pandas(\n        X_train.apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32)\n    )\n    X_val_gpu = cudf.DataFrame.from_pandas(\n        X_val.apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32)\n    )\n\n    # Conversiones para y (objetivo)\n    y_train_np = np.asarray(y_train, dtype=np.float32)\n    y_val_np = np.asarray(y_val, dtype=np.float32)\n\n    # ------------------------------------------\n    # Caso 1Ô∏è‚É£: problema multi-output (por ejemplo, predicci√≥n de dos coordenadas)\n    # ------------------------------------------\n    if y_train_np.ndim == 2 and y_train_np.shape[1] > 1:\n        preds_list = []\n\n        # Entrenar un modelo independiente por cada salida\n        for i in range(y_train_np.shape[1]):\n            y_train_col = cudf.Series(y_train_np[:, i])\n            model = LinearSVR(C=C, epsilon=epsilon, max_iter=1000)\n            model.fit(X_train_gpu, y_train_col)\n            preds = model.predict(X_val_gpu)\n            preds_list.append(preds.to_numpy())\n\n        # Combinar predicciones y evaluar con funci√≥n multi-output\n        preds_combined = np.stack(preds_list, axis=1)\n        return score_mse_multioutput(y_val_np, preds_combined)\n\n    # ------------------------------------------\n    # Caso 2Ô∏è‚É£: problema single-output (una sola variable objetivo)\n    # ------------------------------------------\n    else:\n        y_train_gpu = cudf.Series(y_train_np)\n        y_val_gpu = cudf.Series(y_val_np)\n\n        model = LinearSVR(C=C, epsilon=epsilon, max_iter=1000)\n        model.fit(X_train_gpu, y_train_gpu)\n        preds = model.predict(X_val_gpu)\n\n        preds_np = preds.to_numpy()\n        y_val_np = y_val_gpu.to_numpy()\n\n        return score_mse(y_val_np, preds_np)\n\n\n# ------------------------------------------\n# üîç Creaci√≥n y ejecuci√≥n del estudio de Optuna\n# ------------------------------------------\nstudy_linear_svr = optuna.create_study(direction='minimize')\nstudy_linear_svr.optimize(objective_linear_svr, n_trials=20, show_progress_bar=True)\n\n# ------------------------------------------\n# üìä Resultados finales\n# ------------------------------------------\nprint(\"üèÅ Mejor resultado LinearSVR:\", study_linear_svr.best_value)\nprint(\"üîß Par√°metros √≥ptimos:\", study_linear_svr.best_params)\n\n# Guardar resultados en un registro global (por ejemplo, diccionario o archivo)\nsave_best_params(\"LinearSVR\", study_linear_svr.best_params, study_linear_svr.best_value)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T02:20:17.991933Z","iopub.execute_input":"2025-10-22T02:20:17.992749Z","iopub.status.idle":"2025-10-22T02:20:58.147026Z","shell.execute_reply.started":"2025-10-22T02:20:17.992721Z","shell.execute_reply":"2025-10-22T02:20:58.146193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_memory()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T02:00:21.791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ==========================================\n# ‚ö° Optimizaci√≥n Bayesiana: RandomForestRegressor (cuML + GPU)\n# ==========================================\nimport cudf                 # Manejo de DataFrames en GPU (alternativa de pandas para CUDA)\nimport cupy as cp           # Numpy en GPU\nimport numpy as np\nimport optuna               # Librer√≠a para optimizaci√≥n bayesiana\nfrom cuml.ensemble import RandomForestRegressor  # Random Forest acelerado en GPU\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error   # M√©trica de evaluaci√≥n\n\n# ------------------------------------------\n# üéØ Funci√≥n de puntuaci√≥n para salida m√∫ltiple\n# ------------------------------------------\ndef score_mse_multioutput(y_true, y_pred):\n    \"\"\"\n    Calcula el Error Cuadr√°tico Medio (MSE) para modelos con m√∫ltiples salidas.\n    En caso de tener m√°s de una variable objetivo (por ejemplo, coordenadas X e Y),\n    se calcula el MSE por cada una y se obtiene el promedio.\n    \"\"\"\n\n    # Convertir posibles DataFrames/Series a arreglos numpy\n    if isinstance(y_true, (pd.DataFrame, pd.Series)):\n        y_true = y_true.values\n    if isinstance(y_pred, (pd.DataFrame, pd.Series)):\n        y_pred = y_pred.values\n\n    # Validaci√≥n de dimensiones\n    if y_true.shape != y_pred.shape:\n        if y_true.ndim == 2 and y_pred.ndim == 1:\n            raise ValueError(\"Prediction is 1D, but true values are 2D.\")\n        if y_true.shape[0] != y_pred.shape[0]:\n            raise ValueError(f\"Shape mismatch: y_true {y_true.shape[0]}, y_pred {y_pred.shape[0]}\")\n\n    # Si solo hay una salida\n    if y_true.ndim == 1 or y_true.shape[1] == 1:\n        return mean_squared_error(y_true, y_pred)\n    else:\n        # Si hay dos salidas, se calcula el MSE por cada componente y se promedia\n        mse_x = mean_squared_error(y_true[:, 0], y_pred[:, 0])\n        mse_y = mean_squared_error(y_true[:, 1], y_pred[:, 1])\n        return (mse_x + mse_y) / 2.0\n\n\n# ------------------------------------------\n# üß† Funci√≥n objetivo para Optuna\n# ------------------------------------------\ndef objective_rf(trial):\n    \"\"\"\n    Funci√≥n objetivo para Optuna.\n    Entrena un modelo RandomForestRegressor con distintos hiperpar√°metros\n    y devuelve el MSE promedio sobre el conjunto de validaci√≥n.\n    \"\"\"\n\n    # Hiperpar√°metros a optimizar\n    n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)  # N¬∫ de √°rboles\n    max_depth = trial.suggest_int(\"max_depth\", 3, 15)           # Profundidad m√°xima del √°rbol\n\n    # Convertir los conjuntos de entrenamiento y validaci√≥n a formato GPU (cuDF)\n    X_train_gpu = cudf.DataFrame.from_pandas(\n        X_train.apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32)\n    )\n    X_val_gpu = cudf.DataFrame.from_pandas(\n        X_val.apply(pd.to_numeric, errors='coerce').fillna(0).astype(np.float32)\n    )\n\n    # Convertir las salidas a numpy de 32 bits\n    y_train_np = np.asarray(y_train, dtype=np.float32)\n    y_val_np = np.asarray(y_val, dtype=np.float32)\n\n    # ------------------------------------------\n    # üîÄ Entrenamiento multioutput (dos salidas)\n    # ------------------------------------------\n    # Se entrena un modelo independiente para cada componente de la salida\n    model_x = RandomForestRegressor(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        random_state=42\n    )\n    model_y = RandomForestRegressor(\n        n_estimators=n_estimators,\n        max_depth=max_depth,\n        random_state=42\n    )\n\n    # Conversi√≥n de etiquetas por salida a GPU\n    y_train_x_gpu = cudf.Series(y_train_np[:, 0])\n    y_train_y_gpu = cudf.Series(y_train_np[:, 1])\n\n    # Entrenamiento y predicci√≥n por componente\n    model_x.fit(X_train_gpu, y_train_x_gpu)\n    preds_x = model_x.predict(X_val_gpu)\n\n    model_y.fit(X_train_gpu, y_train_y_gpu)\n    preds_y = model_y.predict(X_val_gpu)\n\n    # Combinar ambas predicciones para evaluar el desempe√±o conjunto\n    preds_combined = np.stack([preds_x.to_numpy(), preds_y.to_numpy()], axis=1)\n\n    # Devolver el MSE promedio como m√©trica a minimizar\n    return score_mse_multioutput(y_val_np, preds_combined)\n\n\n# ------------------------------------------\n# üîç Ejecuci√≥n del estudio con Optuna\n# ------------------------------------------\nstudy_rf = optuna.create_study(direction='minimize')\nstudy_rf.optimize(objective_rf, n_trials=10, show_progress_bar=True)\n\n# ------------------------------------------\n# üìä Resultados finales\n# ------------------------------------------\nprint(\"üèÅ Mejor resultado RandomForest:\", study_rf.best_value)\nprint(\"üîß Par√°metros √≥ptimos:\", study_rf.best_params)\n\n# Guardar los par√°metros √≥ptimos y su rendimiento\nsave_best_params(\"RandomForest\", study_rf.best_params, study_rf.best_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:23:24.354544Z","iopub.execute_input":"2025-10-22T03:23:24.354827Z","iopub.status.idle":"2025-10-22T03:48:10.750378Z","shell.execute_reply.started":"2025-10-22T03:23:24.354806Z","shell.execute_reply":"2025-10-22T03:48:10.749700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_memory()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T02:00:21.791Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## CPU","metadata":{}},{"cell_type":"code","source":"'''\n# ==========================================\n# üß† Optimizaci√≥n Bayesiana: Bayesian Ridge Regression (CPU)\n# ==========================================\nimport optuna\nimport numpy as np\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd  # Asegura compatibilidad con los DataFrames usados\n\n# ------------------------------------------\n# üéØ Funci√≥n de puntuaci√≥n para salida m√∫ltiple\n# ------------------------------------------\ndef score_mse_multioutput(y_true, y_pred):\n    \"\"\"\n    Calcula el Error Cuadr√°tico Medio (MSE) promedio para problemas de m√∫ltiples salidas.\n    Permite evaluar modelos que predicen m√°s de una variable objetivo (por ejemplo: coordenadas X e Y).\n    \"\"\"\n\n    # Convertir posibles DataFrames o Series a arrays de NumPy\n    if isinstance(y_true, (pd.DataFrame, pd.Series)):\n        y_true = y_true.values\n    if isinstance(y_pred, (pd.DataFrame, pd.Series)):\n        y_pred = y_pred.values\n\n    # Validar que las dimensiones de verdad y predicci√≥n sean iguales\n    if y_true.shape != y_pred.shape:\n        if y_true.ndim == 2 and y_pred.ndim == 1:\n            raise ValueError(\"La predicci√≥n es 1D, pero los valores verdaderos son 2D.\")\n        if y_true.shape[0] != y_pred.shape[0]:\n            raise ValueError(f\"Dimensiones incompatibles: y_true {y_true.shape[0]}, y_pred {y_pred.shape[0]}\")\n        if y_true.ndim == 2 and y_pred.ndim == 2 and y_true.shape[1] != y_pred.shape[1]:\n            print(f\"‚ö†Ô∏è Advertencia: diferencia en el n√∫mero de columnas: y_true {y_true.shape}, y_pred {y_pred.shape}\")\n            raise ValueError(\"Las dimensiones de salida no coinciden.\")\n\n    # Caso de una sola salida\n    if y_true.ndim == 1 or y_true.shape[1] == 1:\n        return mean_squared_error(y_true, y_pred)\n    else:\n        # Caso multisalida: se calcula el MSE por cada columna y luego se promedia\n        mse_x = mean_squared_error(y_true[:, 0], y_pred[:, 0])\n        mse_y = mean_squared_error(y_true[:, 1], y_pred[:, 1])\n        return (mse_x + mse_y) / 2.0\n\n\n# ------------------------------------------\n# ‚öôÔ∏è Funci√≥n objetivo para Optuna\n# ------------------------------------------\ndef objective_bayes(trial):\n    \"\"\"\n    Define el proceso de optimizaci√≥n de hiperpar√°metros para el modelo BayesianRidge.\n    Entrena modelos separados para cada variable de salida (por ejemplo, X e Y).\n    \"\"\"\n\n    # Hiperpar√°metros a optimizar mediante b√∫squeda bayesiana\n    alpha_1 = trial.suggest_float(\"alpha_1\", 1e-7, 1e-3, log=True)   # Prior sobre el par√°metro alpha\n    lambda_1 = trial.suggest_float(\"lambda_1\", 1e-7, 1e-3, log=True) # Prior sobre el par√°metro lambda\n\n    # Crear dos modelos independientes para cada componente de la salida\n    model_x = BayesianRidge(alpha_1=alpha_1, lambda_1=lambda_1)\n    model_y = BayesianRidge(alpha_1=alpha_1, lambda_1=lambda_1)\n\n    # Entrenar modelo para la primera variable (X)\n    model_x.fit(X_train, y_train.iloc[:, 0])\n    preds_x = model_x.predict(X_val)\n\n    # Entrenar modelo para la segunda variable (Y)\n    model_y.fit(X_train, y_train.iloc[:, 1])\n    preds_y = model_y.predict(X_val)\n\n    # Combinar las predicciones de ambas salidas en una sola matriz\n    preds_combined = np.stack([preds_x, preds_y], axis=1)\n\n    # Calcular el MSE combinado\n    return score_mse_multioutput(y_val.to_numpy(), preds_combined)\n\n\n# ------------------------------------------\n# üîç Ejecuci√≥n del estudio de Optuna\n# ------------------------------------------\nstudy_bayes = optuna.create_study(direction='minimize')\nstudy_bayes.optimize(objective_bayes, n_trials=15, show_progress_bar=True)\n\n# ------------------------------------------\n# üìä Resultados finales\n# ------------------------------------------\nprint(\"üèÅ Mejor resultado BayesianRidge:\", study_bayes.best_value)\nprint(\"üîß Par√°metros √≥ptimos:\", study_bayes.best_params)\n\n# Guardar los mejores par√°metros obtenidos\nsave_best_params(\"BayesianRidge\", study_bayes.best_params, study_bayes.best_value)\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T02:00:21.791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_memory()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T02:00:21.791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# ==========================================\n# üß† Optimizaci√≥n Bayesiana: Kernel Ridge\n# ==========================================\nfrom sklearn.kernel_ridge import KernelRidge\n\ndef objective_kernel_ridge(trial):\n    alpha = trial.suggest_float(\"alpha\", 1e-3, 10.0, log=True)\n    kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"poly\", \"rbf\"])\n    gamma = trial.suggest_float(\"gamma\", 1e-4, 1.0, log=True)\n\n    model = KernelRidge(alpha=alpha, kernel=kernel, gamma=gamma)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_val)\n    return score_mse(y_val, preds)\n\nstudy_kr = optuna.create_study(direction='minimize')\nstudy_kr.optimize(objective_kernel_ridge, n_trials=25, show_progress_bar=True)\n\nprint(\"üèÅ Mejor resultado Kernel Ridge:\", study_kr.best_value)\nprint(\"üîß Par√°metros:\", study_kr.best_params)\n\nsave_best_params(\"KernelRidge\", study_kr.best_params, study_kr.best_value)\n'''","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T02:00:21.791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_memory()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T02:00:21.791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ==========================================\n# ==========================================\n# üåå Optimizaci√≥n Bayesiana: Gaussian Process Regressor (versi√≥n segura)\n# ==========================================\nimport gc\nimport numpy as np\nimport pandas as pd\nimport optuna\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic\n\n# üß© Reducir tama√±o de dataset para evitar explosi√≥n de RAM\ndef get_small_sample(X, y, frac=0.25):\n    \"\"\"\n    Esta funci√≥n toma una fracci√≥n del dataset original para evitar problemas de memoria\n    o lentitud extrema durante el entrenamiento del modelo Gaussian Process.\n    Los Gaussian Process escalan muy mal con el n√∫mero de muestras (> 5000 puntos puede ser problem√°tico).\n    \"\"\"\n    if len(X) > 5000:\n        # Si el dataset tiene m√°s de 5000 filas, se toma una muestra aleatoria del 25%\n        X_sample = X.sample(frac=frac, random_state=42)\n        y_sample = y.loc[X_sample.index]\n    else:\n        # Si el dataset es peque√±o, se usa completo\n        X_sample, y_sample = X, y\n    return X_sample, y_sample\n\n# üìâ Aplicar reducci√≥n a los conjuntos de entrenamiento y validaci√≥n\nX_train_small, y_train_small = get_small_sample(X_train, y_train)\nX_val_small, y_val_small = get_small_sample(X_val, y_val)\n\n# üåå Definir la funci√≥n objetivo de Optuna\ndef objective_gp(trial):\n    \"\"\"\n    Funci√≥n objetivo que Optuna usar√° para probar diferentes configuraciones del modelo.\n    Retorna el error cuadr√°tico medio (MSE) para cada conjunto de hiperpar√°metros.\n    \"\"\"\n\n    # Optuna probar√° distintos tipos de kernel\n    kernel_choice = trial.suggest_categorical(\n        \"kernel\", [\"RBF\", \"Matern\", \"RationalQuadratic\"]\n    )\n\n    # El par√°metro length_scale controla la suavidad de la funci√≥n del kernel\n    length_scale = trial.suggest_float(\"length_scale\", 0.1, 10.0, log=True)\n\n    # Selecci√≥n del kernel seg√∫n el valor elegido por Optuna\n    if kernel_choice == \"RBF\":\n        kernel = RBF(length_scale=length_scale)\n    elif kernel_choice == \"Matern\":\n        kernel = Matern(length_scale=length_scale, nu=1.5)\n    else:\n        kernel = RationalQuadratic(length_scale=length_scale, alpha=1.0)\n\n    # ‚öôÔ∏è Configurar el modelo Gaussian Process\n    model = GaussianProcessRegressor(\n        kernel=kernel,\n        n_restarts_optimizer=1,  # reduce el tiempo de optimizaci√≥n (menos reinicios)\n        alpha=1e-3,              # agrega un t√©rmino de ruido para evitar singularidades num√©ricas\n        normalize_y=True          # normaliza las etiquetas (ayuda en datasets no centrados)\n    )\n\n    # Entrenar el modelo con el dataset reducido\n    model.fit(X_train_small, y_train_small)\n\n    # Realizar predicciones sobre el conjunto de validaci√≥n reducido\n    preds = model.predict(X_val_small)\n\n    # Calcular el MSE con la funci√≥n score_mse definida previamente\n    return score_mse(y_val_small, preds)\n\n# üöÄ Limpieza antes de ejecutar\nclean_memory()  # Libera memoria de CPU y GPU antes de iniciar el estudio\n\n# üéØ Crear estudio Optuna para minimizar el MSE\nstudy_gp = optuna.create_study(direction='minimize')\n\n# Ejecutar optimizaci√≥n con 8 pruebas (trials)\n# Se usa n_jobs=1 para evitar errores de concurrencia o sobrecarga de memoria\nstudy_gp.optimize(objective_gp, n_trials=8, n_jobs=1, show_progress_bar=True)\n\n# üèÅ Mostrar resultados del mejor modelo\nprint(\"üèÅ Mejor resultado Gaussian Process:\", study_gp.best_value)\nprint(\"üîß Par√°metros:\", study_gp.best_params)\n\n# üíæ Guardar resultados para registro posterior\nsave_best_params(\"GaussianProcessRegressor\", study_gp.best_params, study_gp.best_value)\n\n# üßπ Limpieza final de memoria\nclean_memory()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T03:53:24.571255Z","iopub.execute_input":"2025-10-22T03:53:24.571933Z","execution_failed":"2025-10-22T04:01:40.021Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clean_memory()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-22T02:00:21.792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==========================================\n# ‚ö° OPTIMIZACI√ìN BAYESIANA: XGBoost (CPU)\n# ==========================================\n# En este script se utiliza la librer√≠a Optuna para realizar la optimizaci√≥n bayesiana\n# de los hiperpar√°metros del modelo XGBoost (versi√≥n CPU). El objetivo es encontrar\n# la mejor configuraci√≥n que minimice el error cuadr√°tico medio (MSE) en un conjunto\n# de validaci√≥n, manteniendo eficiencia en el uso de memoria.\n\n# ------------------------------------------------------\n# üì¶ Importaci√≥n de librer√≠as necesarias\n# ------------------------------------------------------\nimport gc                      # Permite la recolecci√≥n manual de basura (limpieza de memoria)\nimport optuna                  # Librer√≠a para optimizaci√≥n autom√°tica de hiperpar√°metros\nimport numpy as np             # Manipulaci√≥n num√©rica\nimport pandas as pd            # Manipulaci√≥n de datos tabulares\nfrom xgboost import XGBRegressor\nfrom xgboost.callback import EarlyStopping  # (Opcional) Callback para detener entrenamiento si no mejora\n\n# ------------------------------------------------------\n# üß© Funci√≥n para reducir el tama√±o del dataset\n# ------------------------------------------------------\ndef get_small_sample(X, y, frac=0.3, max_rows=10000):\n    \"\"\"\n    Toma una muestra del conjunto de datos original para acelerar el proceso de optimizaci√≥n.\n    Esto permite que cada prueba de Optuna se ejecute m√°s r√°pido sin necesidad de usar\n    el 100% de los datos.\n    - frac: porcentaje de filas a tomar si el dataset es peque√±o.\n    - max_rows: l√≠mite m√°ximo de filas para datasets grandes.\n    \"\"\"\n    if len(X) > max_rows:\n        # Si el dataset es muy grande, se toman m√°ximo 10,000 filas aleatorias.\n        X_sample = X.sample(n=max_rows, random_state=42, replace=False)\n        y_sample = y.loc[X_sample.index]\n    else:\n        # Si no es tan grande, se toma una fracci√≥n (por defecto, 30%).\n        X_sample = X.sample(frac=frac, random_state=42, replace=False)\n        y_sample = y.loc[X_sample.index]\n    return X_sample, y_sample\n\n# ------------------------------------------------------\n# üîπ Creaci√≥n de subconjuntos reducidos de entrenamiento y validaci√≥n\n# ------------------------------------------------------\n# Esto hace que la optimizaci√≥n bayesiana sea m√°s liviana y r√°pida.\nX_train_small, y_train_small = get_small_sample(X_train, y_train)\nX_val_small, y_val_small = get_small_sample(X_val, y_val)\n\n# ------------------------------------------------------\n# üéØ Funci√≥n objetivo para Optuna\n# ------------------------------------------------------\ndef objective_xgb(trial):\n    \"\"\"\n    Define los hiperpar√°metros que ser√°n optimizados por Optuna.\n    Cada 'trial' representa una combinaci√≥n diferente de par√°metros.\n    La funci√≥n devuelve el valor del MSE obtenido en validaci√≥n, que es la m√©trica a minimizar.\n    \"\"\"\n    # üîß Espacio de b√∫squeda de hiperpar√°metros\n    params = {\n        \"tree_method\": \"hist\",          # Implementaci√≥n eficiente para CPU\n        \"predictor\": \"cpu_predictor\",   # Fuerza el uso de CPU (no GPU)\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),  # Tasa de aprendizaje\n        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 9),                          # Profundidad m√°xima de los √°rboles\n        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),                   # Fracci√≥n de muestras usadas en cada √°rbol\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),     # Fracci√≥n de caracter√≠sticas usadas\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 80, 300),                # N√∫mero de √°rboles\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 10.0, log=True),     # Regularizaci√≥n L2\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 10.0, log=True),       # Regularizaci√≥n L1\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),          # Peso m√≠nimo por nodo hoja\n        \"verbosity\": 0,                                                            # Silencia mensajes internos\n        \"random_state\": 42                                                         # Semilla para reproducibilidad\n    }\n\n    # ‚öôÔ∏è Entrenamiento del modelo con los par√°metros propuestos\n    model = XGBRegressor(**params)\n\n    model.fit(\n        X_train_small,                      # Datos de entrenamiento reducidos\n        y_train_small,                      # Etiquetas correspondientes\n        eval_set=[(X_val_small, y_val_small)],  # Conjunto de validaci√≥n para evaluaci√≥n interna\n        verbose=False\n    )\n\n    # üîç Predicci√≥n sobre el conjunto de validaci√≥n\n    preds = model.predict(X_val_small)\n\n    # üìè C√°lculo del error cuadr√°tico medio (MSE)\n    score = score_mse(y_val_small, preds)\n\n    # üßπ Limpieza de memoria entre iteraciones para evitar saturaci√≥n\n    clean_memory()\n    del model, preds\n    return score  # Retorna el valor del error (que Optuna intentar√° minimizar)\n\n# ------------------------------------------------------\n# üöÄ Limpieza de memoria antes de iniciar el estudio\n# ------------------------------------------------------\nclean_memory()\n\n# ------------------------------------------------------\n# üìä Creaci√≥n y ejecuci√≥n del estudio de Optuna\n# ------------------------------------------------------\n# 'direction=minimize' indica que Optuna buscar√° el valor m√°s bajo del MSE.\n# 'n_trials' define cu√°ntas combinaciones de par√°metros se probar√°n.\n# 'n_jobs=1' mantiene la ejecuci√≥n en serie para evitar conflictos de memoria.\nstudy_xgb = optuna.create_study(direction='minimize')\nstudy_xgb.optimize(objective_xgb, n_trials=15, n_jobs=1, show_progress_bar=True)\n\n# ------------------------------------------------------\n# üìà Resultados del estudio\n# ------------------------------------------------------\n# Se muestran los mejores hiperpar√°metros y el valor del MSE obtenido.\nprint(\"üèÅ Mejor resultado XGBoost (CPU):\", study_xgb.best_value)\nprint(\"üîß Par√°metros:\", study_xgb.best_params)\n\n# ------------------------------------------------------\n# üíæ Guardar los par√°metros y resultado √≥ptimo\n# ------------------------------------------------------\n# Se almacenan los resultados para poder consultarlos o reutilizarlos posteriormente.\nsave_best_params(\"XGBoost_CPU\", study_xgb.best_params, study_xgb.best_value)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T02:21:41.086012Z","iopub.execute_input":"2025-10-22T02:21:41.086563Z","iopub.status.idle":"2025-10-22T02:22:15.442157Z","shell.execute_reply.started":"2025-10-22T02:21:41.086534Z","shell.execute_reply":"2025-10-22T02:22:15.441275Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# üß† Justificaci√≥n de la selecci√≥n de hiperpar√°metros y rejillas de optimizaci√≥n\n\n## üéØ Objetivo general\nEl proceso de optimizaci√≥n bayesiana se aplic√≥ a diferentes modelos de regresi√≥n con el fin de **minimizar el error cuadr√°tico medio (MSE)**.  \nLos hiperpar√°metros y sus rangos fueron definidos considerando la naturaleza del algoritmo, el riesgo de sobreajuste y la capacidad de generalizaci√≥n de cada modelo.\n\n---\n\n## üå≤ XGBoost (CPU)\n\n**Hiperpar√°metros optimizados:**\n- `learning_rate` ‚àà [0.01, 0.3]  \n- `max_depth` ‚àà [3, 10]  \n- `subsample` ‚àà [0.5, 1.0]  \n- `colsample_bytree` ‚àà [0.5, 1.0]  \n- `n_estimators` ‚àà [100, 500]  \n- `reg_lambda` ‚àà [1e-3, 10.0]  \n- `reg_alpha` ‚àà [1e-3, 10.0]  \n- `min_child_weight` ‚àà [1, 10]\n\n**Justificaci√≥n:**  \nXGBoost es un modelo basado en **√°rboles potenciados (boosting)** sensible a los par√°metros de profundidad y regularizaci√≥n.  \n- `learning_rate` controla la tasa de aprendizaje; valores bajos favorecen estabilidad.  \n- `max_depth` evita sobreajuste limitando la complejidad de cada √°rbol.  \n- `subsample` y `colsample_bytree` incrementan la diversidad del ensamble.  \n- `reg_lambda`, `reg_alpha` y `min_child_weight` regulan la penalizaci√≥n de la complejidad del modelo.  \n\n**üîπ Resultado:**  \nüìä *MSE:* 107.38  \n‚öôÔ∏è *Par√°metros √≥ptimos:* `{'learning_rate': 0.0242, 'max_depth': 9, 'subsample': 0.676, 'colsample_bytree': 0.997, 'n_estimators': 298, 'reg_lambda': 3.26, 'reg_alpha': 0.82, 'min_child_weight': 3}`\n\n---\n\n## üìà Linear Regression\n\n**Hiperpar√°metro optimizado:**  \n- `fit_intercept` ‚àà {True, False}\n\n**Justificaci√≥n:**  \nEste modelo solo requiere definir si se incluye o no el intercepto.  \nEl resultado `fit_intercept=False` indica que los datos se encontraban **centrados** alrededor del origen.  \nEl MSE alto demuestra que el fen√≥meno no es lineal, pero el modelo sirve como **baseline** para comparar otros.\n\n**üîπ Resultado:**  \nüìä *MSE:* 1,135,150,804.38  \n‚öôÔ∏è *Par√°metros √≥ptimos:* `{'fit_intercept': False}`\n\n---\n\n## üß© Lasso Regression\n\n**Hiperpar√°metro optimizado:**  \n- `alpha` ‚àà [1e-3, 10]\n\n**Justificaci√≥n:**  \n`alpha` regula la **penalizaci√≥n L1**, que introduce sparsidad (reduce coeficientes a cero).  \nUn rango logar√≠tmico amplio permiti√≥ explorar regularizaciones d√©biles y fuertes.  \nEl valor `Œ± ‚âà 0.10` muestra una regularizaci√≥n moderada que evita el sobreajuste.\n\n**üîπ Resultado:**  \nüìä *MSE:* 170.48  \n‚öôÔ∏è *Par√°metros √≥ptimos:* `{'alpha': 0.102}`\n\n---\n\n## üßÆ Ridge Regression\n\n**Hiperpar√°metro optimizado:**  \n- `alpha` ‚àà [1e-3, 10]\n\n**Justificaci√≥n:**  \n`alpha` regula la **penalizaci√≥n L2**, que suaviza los coeficientes sin eliminarlos.  \nEl valor `Œ± ‚âà 8.48` refleja una regularizaci√≥n fuerte adecuada ante **multicolinealidad**.  \nEsto reduce la varianza del modelo y mejora su estabilidad.\n\n**üîπ Resultado:**  \nüìä *MSE:* 194.21  \n‚öôÔ∏è *Par√°metros √≥ptimos:* `{'alpha': 8.485}`\n\n---\n\n## ‚öñÔ∏è ElasticNet\n\n**Hiperpar√°metros optimizados:**  \n- `alpha` ‚àà [1e-3, 1]  \n- `l1_ratio` ‚àà [0, 1]\n\n**Justificaci√≥n:**  \nCombina Lasso (L1) y Ridge (L2).  \n- `alpha` define la fuerza total de regularizaci√≥n.  \n- `l1_ratio` equilibra sparsidad y suavidad.  \nEl valor `l1_ratio ‚âà 0.63` sugiere predominio leve de la penalizaci√≥n L1.\n\n**üîπ Resultado:**  \nüìä *MSE:* 170.55  \n‚öôÔ∏è *Par√°metros √≥ptimos:* `{'alpha': 0.0082, 'l1_ratio': 0.631}`\n\n---\n\n## ‚öôÔ∏è Linear SVR\n\n**Hiperpar√°metros optimizados:**  \n- `C` ‚àà [0.1, 10]  \n- `epsilon` ‚àà [0.001, 1]\n\n**Justificaci√≥n:**  \n`C` controla la penalizaci√≥n de errores: valores altos reducen sesgo pero aumentan varianza.  \n`epsilon` define la tolerancia del margen donde los errores no se penalizan.  \nEl resultado `C ‚âà 9.42` y `Œµ ‚âà 0.13` indica alta flexibilidad pero sensibilidad al ruido.\n\n**üîπ Resultado:**  \nüìä *MSE:* 2269.93  \n‚öôÔ∏è *Par√°metros √≥ptimos:* `{'C': 9.423, 'epsilon': 0.1295}`\n\n---\n\n## üå≥ Random Forest (cuML - GPU)\n\n**Hiperpar√°metros optimizados:**  \n- `n_estimators` ‚àà [100, 500]  \n- `max_depth` ‚àà [3, 15]\n\n**Justificaci√≥n:**  \nLos bosques aleatorios combinan m√∫ltiples √°rboles sobre subconjuntos aleatorios de los datos.  \n- `n_estimators` controla el n√∫mero de √°rboles, equilibrando precisi√≥n y costo computacional.  \n- `max_depth` regula la complejidad de cada √°rbol, previniendo sobreajuste.  \nEl modelo con `n_estimators=346` y `max_depth=15` mostr√≥ excelente equilibrio entre sesgo y varianza.\n\n**üîπ Resultado:**  \nüìä *MSE:* 109.19  \n‚öôÔ∏è *Par√°metros √≥ptimos:* `{'n_estimators': 346, 'max_depth': 15}`\n\n---\n\n## üìä Conclusi√≥n general\n\n- Los rangos de hiperpar√°metros fueron definidos seg√∫n la **naturaleza de cada algoritmo** (regularizaci√≥n, profundidad, tasa de aprendizaje, etc.).  \n- Se usaron escalas **logar√≠tmicas** para variables continuas sensibles como `alpha`, `C` o `learning_rate`.  \n- El objetivo fue **minimizar el MSE**, priorizando modelos con bajo error cuadr√°tico medio y buena capacidad de generalizaci√≥n.  \n- Los modelos basados en **√°rboles (XGBoost y RandomForest)** mostraron el mejor desempe√±o, indicando una **relaci√≥n no lineal** entre las variables independientes y la variable objetivo.\n\n---","metadata":{}},{"cell_type":"markdown","source":"## üß™ Evaluaci√≥n final de los modelos optimizados\n\nEn esta secci√≥n se eval√∫an los modelos con los mejores hiperpar√°metros encontrados mediante **optimizaci√≥n bayesiana**, utilizando el conjunto de **evaluaci√≥n (test)**.\n\nLas m√©tricas empleadas para medir el desempe√±o son:\n\n- **MAE (Mean Absolute Error)**: mide el error absoluto medio entre valores reales y predichos.  \n- **MSE (Mean Squared Error)**: mide el error cuadr√°tico medio, penalizando m√°s los errores grandes.  \n- **R¬≤ (Coeficiente de determinaci√≥n)**: mide la proporci√≥n de la varianza explicada por el modelo.  \n- **MAPE (Mean Absolute Percentage Error)**: mide el error porcentual medio entre las predicciones y los valores reales.\n\nEstas m√©tricas permiten comparar objetivamente el rendimiento y estabilidad de cada modelo.","metadata":{}},{"cell_type":"code","source":"# ==========================================\n# üìä Evaluaci√≥n final (solo primeros 5 modelos optimizados)\n# ==========================================\n# En esta secci√≥n se realiza la evaluaci√≥n comparativa de los primeros 5 modelos\n# de regresi√≥n optimizados (Linear Regression, Lasso, Ridge, ElasticNet, XGBoost).\n# Se utiliza validaci√≥n cruzada K-Fold para obtener m√©tricas promedio de desempe√±o.\n\n# ------------------------------------------------------\n# üì¶ Importaci√≥n de librer√≠as\n# ------------------------------------------------------\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom tqdm import tqdm  # Para mostrar barra de progreso en los bucles\n\n# ------------------------------------------------------\n# üìè Definici√≥n de la m√©trica MAPE\n# ------------------------------------------------------\ndef mean_absolute_percentage_error(y_true, y_pred):\n    \"\"\"\n    Calcula el Error Porcentual Absoluto Medio (MAPE), expresado en porcentaje.\n    Esta m√©trica mide el error relativo promedio entre las predicciones y los valores reales.\n    - y_true: valores verdaderos\n    - y_pred: predicciones del modelo\n    \"\"\"\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    # Se usa np.clip para evitar divisiones por cero\n    return np.mean(np.abs((y_true - y_pred) / np.clip(np.abs(y_true), 1e-10, None))) * 100\n\n# ------------------------------------------------------\n# üîÅ Funci√≥n de evaluaci√≥n con validaci√≥n cruzada (K-Fold)\n# ------------------------------------------------------\ndef evaluar_modelo(model_class, params, X, y, n_splits=5):\n    \"\"\"\n    Eval√∫a un modelo usando validaci√≥n cruzada K-Fold.\n    Entrena el modelo en distintos subconjuntos del conjunto de datos\n    y promedia las m√©tricas obtenidas para una estimaci√≥n m√°s robusta.\n\n    Par√°metros:\n        - model_class: clase del modelo (ej. LinearRegression, XGBRegressor)\n        - params: diccionario con los hiperpar√°metros √≥ptimos\n        - X, y: datos de entrada y etiquetas\n        - n_splits: n√∫mero de particiones K para el K-Fold (por defecto 5)\n\n    Retorna:\n        Un diccionario con la media y desviaci√≥n est√°ndar de las m√©tricas:\n        MAE, MSE, R2 y MAPE.\n    \"\"\"\n    # Se inicializa el esquema de validaci√≥n cruzada con barajado aleatorio\n    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n    metrics = {\"MAE\": [], \"MSE\": [], \"R2\": [], \"MAPE\": []}\n\n    # üîÑ Iterar sobre cada divisi√≥n del conjunto de datos\n    for train_idx, test_idx in kf.split(X):\n        # Separar los conjuntos de entrenamiento y prueba\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]\n\n        # üß† Entrenamiento del modelo en el conjunto de entrenamiento\n        model = model_class(**params)\n        model.fit(X_train, y_train_fold)\n\n        # üîÆ Predicciones sobre el conjunto de prueba\n        preds = model.predict(X_test)\n\n        # üìà C√°lculo de m√©tricas para este fold\n        metrics[\"MAE\"].append(mean_absolute_error(y_test_fold, preds))   # Error absoluto medio\n        metrics[\"MSE\"].append(mean_squared_error(y_test_fold, preds))   # Error cuadr√°tico medio\n        metrics[\"R2\"].append(r2_score(y_test_fold, preds))              # Coeficiente de determinaci√≥n\n        metrics[\"MAPE\"].append(mean_absolute_percentage_error(y_test_fold, preds))  # Error porcentual medio\n\n    # üìä C√°lculo de medias y desviaciones est√°ndar\n    resumen = {\n        \"MAE_mean\": np.mean(metrics[\"MAE\"]), \"MAE_std\": np.std(metrics[\"MAE\"]),\n        \"MSE_mean\": np.mean(metrics[\"MSE\"]), \"MSE_std\": np.std(metrics[\"MSE\"]),\n        \"R2_mean\": np.mean(metrics[\"R2\"]), \"R2_std\": np.std(metrics[\"R2\"]),\n        \"MAPE_mean\": np.mean(metrics[\"MAPE\"]), \"MAPE_std\": np.std(metrics[\"MAPE\"]),\n    }\n    return resumen\n\n# ------------------------------------------------------\n# ‚öôÔ∏è Definici√≥n de modelos optimizados (solo los 5 primeros)\n# ------------------------------------------------------\n# Cada entrada del diccionario contiene:\n#   - El nombre del modelo\n#   - Su clase (por ejemplo, Ridge, Lasso, etc.)\n#   - El conjunto de hiperpar√°metros encontrados mediante optimizaci√≥n bayesiana\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nimport xgboost as xgb\n\nmodelos_optimizados = {\n    \"XGBoost_CPU\": (xgb.XGBRegressor, {\n        'learning_rate': 0.02420924374933795, 'max_depth': 9, 'subsample': 0.6761794940794902,\n        'colsample_bytree': 0.9967017297292293, 'n_estimators': 298,\n        'reg_lambda': 3.2612725416927546, 'reg_alpha': 0.8238772544912423,\n        'min_child_weight': 3, 'n_jobs': -1\n    }),\n    \"Linear Regression\": (LinearRegression, {'fit_intercept': False}),\n    \"Lasso\": (Lasso, {'alpha': 0.10200497483254219}),\n    \"Ridge\": (Ridge, {'alpha': 8.485277197307424}),\n    \"ElasticNet\": (ElasticNet, {'alpha': 0.008247107071507926, 'l1_ratio': 0.6308676894445052})\n}\n\n# ------------------------------------------------------\n# üöÄ Evaluaci√≥n de los modelos con barra de progreso\n# ------------------------------------------------------\nresultados = {}\n\nprint(\"üöÄ Iniciando evaluaci√≥n de modelos (primeros 5)...\\n\")\n# tqdm muestra visualmente el avance del proceso de evaluaci√≥n\nfor modelo, (model_class, params) in tqdm(list(modelos_optimizados.items())[:5], desc=\"Progreso\", unit=\"modelo\"):\n    # Se eval√∫a cada modelo con K-Fold y se almacenan los resultados\n    resultados[modelo] = evaluar_modelo(model_class, params, X_test, y_test)\n\n# ------------------------------------------------------\n# üìã Mostrar resultados finales en formato tabla\n# ------------------------------------------------------\n# Se crea un DataFrame con los resultados y se redondean los valores a 4 decimales\ndf_resultados = pd.DataFrame(resultados).T\ndf_resultados = df_resultados.round(4)\n\n# display() permite visualizar la tabla con formato en notebooks\ndisplay(df_resultados)\n\nprint(\"\\n‚úÖ Evaluaci√≥n completada para los primeros 5 modelos. Resultados mostrados arriba.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T02:24:49.594483Z","iopub.execute_input":"2025-10-22T02:24:49.594818Z","iopub.status.idle":"2025-10-22T02:30:31.536397Z","shell.execute_reply.started":"2025-10-22T02:24:49.594793Z","shell.execute_reply":"2025-10-22T02:30:31.534952Z"}},"outputs":[],"execution_count":null}]}